<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#FFFFFF"><title>Neural coding for shape and texture in macaque area V4 &#183; Taekjun Kim</title>
<meta name=title content="Neural coding for shape and texture in macaque area V4 &#183; Taekjun Kim"><script type=text/javascript src=/js/appearance.min.8a082f81b27f3cb2ee528df0b0bdc39787034cf2cc34d4669fbc9977c929023c.js integrity="sha256-iggvgbJ/PLLuUo3wsL3Dl4cDTPLMNNRmn7yZd8kpAjw="></script><link type=text/css rel=stylesheet href=/css/main.bundle.min.bb20018254d048642b9bb4d07c490f792d638246be64872943e3fefe8ef7c064.css integrity="sha256-uyABglTQSGQrm7TQfEkPeS1jgka+ZIcpQ+P+/o73wGQ="><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.af5d9722112bedac95702865c340bcd6286c4e9b2c15ce26b531ea1fba974cb8.js integrity="sha256-r12XIhEr7ayVcChlw0C81ihsTpssFc4mtTHqH7qXTLg=" data-copy=Copy data-copied=Copied></script><meta name=description content="
      
        Taekjun Kim, Wyeth Bair, Anitha Pasupathy Journal of Neuroscience
      
    "><link rel=canonical href=https://taekjunkim.github.io/publications/jneurosci_2019/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:url" content="https://taekjunkim.github.io/publications/jneurosci_2019/"><meta property="og:site_name" content="Taekjun Kim"><meta property="og:title" content="Neural coding for shape and texture in macaque area V4"><meta property="og:description" content="Taekjun Kim, Wyeth Bair, Anitha Pasupathy Journal of Neuroscience"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="publications"><meta property="article:published_time" content="2019-06-12T00:00:00+00:00"><meta property="article:modified_time" content="2019-06-12T00:00:00+00:00"><meta property="article:tag" content="Primate"><meta property="article:tag" content="Texture"><meta property="article:tag" content="Shape"><meta property="article:tag" content="Boundary"><meta property="article:tag" content="Surface"><meta property="article:tag" content="Temporal Dynamics"><meta name=twitter:card content="summary"><meta name=twitter:title content="Neural coding for shape and texture in macaque area V4"><meta name=twitter:description content="Taekjun Kim, Wyeth Bair, Anitha Pasupathy Journal of Neuroscience"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","articleSection":"Selected publications","name":"Neural coding for shape and texture in macaque area V4","headline":"Neural coding for shape and texture in macaque area V4","abstract":"Taekjun Kim, Wyeth Bair, Anitha Pasupathy \u003ccode\u003eJournal of Neuroscience\u003c\/code\u003e","inLanguage":"en","url":"https:\/\/taekjunkim.github.io\/publications\/jneurosci_2019\/","author":{"@type":"Person","name":"Taekjun Kim"},"copyrightYear":"2019","dateCreated":"2019-06-12T00:00:00\u002b00:00","datePublished":"2019-06-12T00:00:00\u002b00:00","dateModified":"2019-06-12T00:00:00\u002b00:00","keywords":["Primate","Texture","Shape","Boundary","Surface","Temporal dynamics","Perception","Electrophysiology","area V4","Vision","Neuroscience"],"mainEntityOfPage":"true","wordCount":"2198"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://taekjunkim.github.io/","name":"","position":1},{"@type":"ListItem","item":"https://taekjunkim.github.io/publications/","name":"Selected Publications","position":2},{"@type":"ListItem","name":"Neural Coding for Shape and Texture in Macaque Area V4","position":3}]}</script><meta name=author content="Taekjun Kim"><link href=mailto:taekjunkim1223@gmail.com rel=me><link href=https://github.com/taekjunkim rel=me><link href=https://linkedin.com/in/taekjun-kim rel=me><link href="https://scholar.google.com/citations?user=pP442rIAAAAJ&amp;hl=en" rel=me></head><body class="m-auto flex h-screen max-w-7xl flex-col bg-neutral px-6 text-lg leading-7 text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"><div id=the-top class="absolute flex self-center"><a class="-translate-y-8 rounded-b-lg bg-primary-200 px-3 py-1 text-sm focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="pe-2 font-bold text-primary-600 dark:text-primary-400">&darr;</span>Skip to main content</a></div><header class="py-6 font-semibold text-neutral-900 dark:text-neutral sm:py-10 print:hidden"><nav class="flex items-start justify-between sm:items-center"><div class="flex flex-row items-center"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2" rel=me href=/>Taekjun Kim</a></div><ul class="flex list-none flex-col text-end sm:flex-row"><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/about/ title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">About Me</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/posts/ title=Posts><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Posts</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/publications/ title="Selected publications"><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Publications</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/tags/ title=Tags><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Tags</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><button id=search-button-1 title="Search (/)">
<span class="group-dark:hover:text-primary-400 transition-colors group-hover:text-primary-600"><span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></span><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"></span></button></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><button id=appearance-switcher-1 type=button aria-label="appearance switcher">
<span class="group-dark:hover:text-primary-400 inline transition-colors group-hover:text-primary-600 dark:hidden" title="Switch to dark appearance"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg>
</span><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"></span>
</span><span class="group-dark:hover:text-primary-400 hidden transition-colors group-hover:text-primary-600 dark:inline" title="Switch to light appearance"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg>
</span><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"></span></span></button></li></ul></nav></header><div class="relative flex grow flex-col"><main id=main-content class=grow><article><header class=max-w-prose><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="hidden inline"><a class="dark:underline-neutral-600 decoration-neutral-300 hover:underline" href=/></a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="dark:underline-neutral-600 decoration-neutral-300 hover:underline" href=/publications/>Selected publications</a><span class="px-1 text-primary-500">/</span></li><li class="hidden inline"><a class="dark:underline-neutral-600 decoration-neutral-300 hover:underline" href=/publications/jneurosci_2019/>Neural coding for shape and texture in macaque area V4</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mb-8 mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Neural coding for shape and texture in macaque area V4</h1><div class="mb-10 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime="2019-06-12 00:00:00 +0000 UTC">12 June 2019</time><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">11 mins</span></div><div class="my-1 flex flex-wrap text-xs leading-relaxed text-neutral-500 dark:text-neutral-400"><a href=/tags/primate/ class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">Primate</a>
<a href=/tags/texture/ class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">Texture</a>
<a href=/tags/shape/ class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">Shape</a>
<a href=/tags/boundary/ class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">Boundary</a>
<a href=/tags/surface/ class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">Surface</a>
<a href=/tags/temporal-dynamics/ class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">Temporal Dynamics</a>
<a href=/tags/perception/ class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">Perception</a>
<a href=/tags/electrophysiology/ class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">Electrophysiology</a>
<a href=/tags/area-v4/ class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">Area V4</a>
<a href=/tags/vision/ class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">Vision</a>
<a href=/tags/neuroscience/ class="mx-1 my-1 rounded-md border border-neutral-200 px-1 py-[1px] hover:border-primary-300 hover:text-primary-700 dark:border-neutral-600 dark:hover:border-primary-600 dark:hover:text-primary-400">Neuroscience</a></div></div></header><section class="prose mt-0 flex max-w-full flex-col dark:prose-invert lg:flex-row"><div class="order-first px-0 lg:order-last lg:max-w-xs lg:ps-8"><div class="toc pe-5 lg:sticky lg:top-10 print:hidden"><details open class="-ms-5 mt-0 overflow-hidden rounded-lg ps-5"><summary class="block cursor-pointer bg-neutral-100 py-1 ps-5 text-lg font-semibold text-neutral-800 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="border-s border-dotted border-neutral-300 py-2 ps-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#article-info>Article info</a></li><li><a href=#abstract>Abstract</a></li><li><a href=#figures>Figures</a><ul><li><a href=#fig1-visual-stimuli>Fig1. Visual stimuli</a></li><li><a href=#fig2-example-neurons-and-population-results>Fig2. Example neurons and population results</a></li><li><a href=#fig3-hmax-model-prediction-of-responses-to-texture-stimuli>Fig3. HMax model prediction of responses to texture stimuli</a></li><li><a href=#fig4-sd-ratios-from-hmax-model-prediction>Fig4. SD ratios from HMax model prediction</a></li><li><a href=#fig5-tuning-for-boundary-curvature-in-shape-selective-neurons>Fig5. Tuning for boundary curvature in shape-selective neurons</a></li><li><a href=#fig6-tuning-for-perceptual-dimension-of-texture>Fig6. Tuning for perceptual dimension of texture</a></li><li><a href=#fig7-joint-coding-of-shape-and-texture>Fig7. Joint coding of shape and texture</a></li><li><a href=#fig8-temporal-dynamics-of-shape-and-texture-selectivity>Fig8. Temporal dynamics of shape and texture selectivity</a></li></ul></li></ul></nav></div></details></div></div><div class="min-h-0 min-w-0 max-w-prose grow"><h2 id=article-info class="relative group">Article info <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#article-info aria-label=Anchor>#</a></span></h2><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><code>Authors</code></td><td>Taekjun Kim, Wyeth Bair, Anitha Pasupathy</td></tr><tr><td><code>Publication date</code></td><td>2019/06/12</td></tr><tr><td><code>Journal</code></td><td>Journal of Neuroscience</td></tr><tr><td><code>DOI</code></td><td><a href=https://doi.org/10.1523/JNEUROSCI.3073-18.2019 target=_blank rel=noreferrer>https://doi.org/10.1523/JNEUROSCI.3073-18.2019</a></td></tr></tbody></table><h2 id=abstract class="relative group">Abstract <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#abstract aria-label=Anchor>#</a></span></h2><p>The distinct visual sensations of shape and texture have been studied separately in cortex; therefore, it remains unknown whether separate neuronal populations encode each of these properties or one population carries a joint encoding. We directly compared shape and texture selectivity of individual V4 neurons in awake macaques (1 male, 1 female) and found that V4 neurons lie along a continuum from strong tuning for boundary curvature of shapes to strong tuning for perceptual dimensions of texture. Among neurons tuned to both attributes, tuning for shape and texture were largely separable, with the latter delayed by ∼30 ms. We also found that shape stimuli typically evoked stronger, more selective responses than did texture patches, regardless of whether the latter were contained within or extended beyond the receptive field. These results suggest that there are separate specializations in mid-level cortical processing for visual attributes of shape and texture.</p><h2 id=figures class="relative group">Figures <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#figures aria-label=Anchor>#</a></span></h2><h3 id=fig1-visual-stimuli class="relative group">Fig1. Visual stimuli <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#fig1-visual-stimuli aria-label=Anchor>#</a></span></h3><p><figure><picture class="mx-auto my-0 rounded-md"><source srcset="/publications/jneurosci_2019/F1.large_hu_1bbc87dce0623f60.webp 330w,/publications/jneurosci_2019/F1.large_hu_257547983b6b0f02.webp 660w
,/publications/jneurosci_2019/F1.large_hu_60fa0708715262f4.webp 1024w
,/publications/jneurosci_2019/F1.large_hu_df0c4db1f583d0f3.webp 1280w" sizes=100vw type=image/webp><img width=1280 height=1094 class="mx-auto my-0 rounded-md" alt=Fig1 loading=lazy decoding=async src=/publications/jneurosci_2019/F1.large_hu_248518819d472ab9.jpg srcset="/publications/jneurosci_2019/F1.large_hu_fb1c40a460c4dd24.jpg 330w,/publications/jneurosci_2019/F1.large_hu_248518819d472ab9.jpg 660w
,/publications/jneurosci_2019/F1.large_hu_4031aeba5c352cdb.jpg 1024w
,/publications/jneurosci_2019/F1.large.jpg 1280w" sizes=100vw></picture></figure><font size=2>Visual stimuli. <strong>A</strong>, Shape stimuli. We used a subset (30 of 51) of the 2D shapes developed by Pasupathy and Connor (2001) to study how boundary conformation influences V4 responses. Most shapes were presented at 8 rotations (45° increments); a few shapes (those identified with a superscript) were presented at fewer rotations (1, 2, or 4, as noted in figure) due to rotational symmetry. The circle was presented at three luminance contrast levels (1, 16, 46 cd/m2) relative to the background (8 cd/m2), for a total of 225 shape stimuli. <strong>B</strong>, Texture stimuli. We constructed eight (23) texture categories based on three dimensions that influence human texture perception (coarse vs fine, directional vs nondirectional, regular vs irregular), and selected 2–3 representative textures for each category (see Materials and Methods). Each texture was presented through a circular aperture of two sizes and at four orientations at 45° increments, for a total of 168 texture stimuli. <strong>C</strong>, Examples of shape, large aperture texture, and small aperture texture stimuli. All parts of the shape stimulus were within the estimated RF region (yellow dotted line). The diameter of the large aperture texture stimuli was twice that of the estimated RF. Small aperture texture condition was created by applying a RF sized circular aperture to large aperture texture.</font></p><h3 id=fig2-example-neurons-and-population-results class="relative group">Fig2. Example neurons and population results <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#fig2-example-neurons-and-population-results aria-label=Anchor>#</a></span></h3><p><figure><picture class="mx-auto my-0 rounded-md"><source srcset="/publications/jneurosci_2019/F2.large_hu_e85badea115d4ddc.webp 330w,/publications/jneurosci_2019/F2.large_hu_ec086e1e5552d771.webp 660w
,/publications/jneurosci_2019/F2.large_hu_53dcb3543e959e.webp 1024w
,/publications/jneurosci_2019/F2.large_hu_ad8506b21cda3981.webp 1280w" sizes=100vw type=image/webp><img width=1280 height=812 class="mx-auto my-0 rounded-md" alt=Fig2 loading=lazy decoding=async src=/publications/jneurosci_2019/F2.large_hu_f12aefd7c272fd56.jpg srcset="/publications/jneurosci_2019/F2.large_hu_8a09e5561842c4ba.jpg 330w,/publications/jneurosci_2019/F2.large_hu_f12aefd7c272fd56.jpg 660w
,/publications/jneurosci_2019/F2.large_hu_9aea3f2d7ffead47.jpg 1024w
,/publications/jneurosci_2019/F2.large.jpg 1280w" sizes=100vw></picture></figure><font size=2><strong>A</strong>, Response frequency histogram for shape (top row) and texture (bottom row) stimuli for 6 example neurons (columns). Red and blue histograms represent responses to small and large aperture textures, respectively. Responses for each neuron were normalized to the maximum across all shape and texture stimuli (maximum values are shown for each neuron). Triangles represent the background responses (no visual stimulus). <strong>B</strong>, Maximum response across all shape stimuli (x axis) is plotted against the maximum response across all small aperture texture stimuli (y axis) for each neuron from Monkey 1 (black) and Monkey 2 (gray). Filled symbols represent neurons with a statistically significant difference between the strongest shape and texture response, assessed with a permutation test (see Materials and Methods). In both monkeys, the maximum response across shape stimuli was typically greater than the maximum response across texture stimuli. <strong>C</strong>, SD of the response frequency histogram for shape (x axis) is plotted against that for texture stimuli (y axis). Asterisks indicate mean value. Yellow highlight represents region where SD ratio for shape versus texture lies between 2/3 and 3/2. Points corresponding to examples in A are identified. <strong>D</strong>, Histogram of the SD ratio: SDshape/SDtexture. Yellow highlight as in <strong>C</strong>. <strong>E, F</strong>, SD values for shuffled shape and texture responses. Shape and texture responses for each neuron were shuffled and SDs were recomputed for two randomly divided groups: Group 1 (N = 225, number of shapes) and Group 2 (N = 84, number of textures).<strong>F</strong>, Gray bars represent SD ratios computed from E. This process was repeated 10,000 times and width of the distribution from the observed data, quantified by the interquartile range of log (SD ratio), was always at least 5 times broader than that from the shuffled data.</font></p><h3 id=fig3-hmax-model-prediction-of-responses-to-texture-stimuli class="relative group">Fig3. HMax model prediction of responses to texture stimuli <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#fig3-hmax-model-prediction-of-responses-to-texture-stimuli aria-label=Anchor>#</a></span></h3><p><figure><picture class="mx-auto my-0 rounded-md"><source srcset=/publications/jneurosci_2019/F3.large_hu_6824f807658666da.webp sizes=100vw type=image/webp><img width=592 height=1280 class="mx-auto my-0 rounded-md" alt=Fig3 loading=lazy decoding=async src=/publications/jneurosci_2019/F3.large.jpg></picture></figure><font size=2><strong>A</strong>, The top 20 preferred shape and texture stimuli of an example neuron (#7). <strong>B</strong>, Shape template for the S2 unit corresponding to the best fitting HMax model based on the responses to shape stimuli. Each ellipse indicates position, orientation, and size of complex-cell like subunit (C1 unit). Grayscale represents weighting strength with darker color denoting stronger weight. <strong>C</strong>, Predicted responses (y axis) based on the best HMax model fit (shown in <strong>B</strong>) for shape (gray) and texture (red) stimuli are plotted against measured responses (x axis). For this neuron, the HMax model provided an excellent fit for shape responses, but not for texture responses. Predicted texture responses showed a much broader range than the observed data. <strong>D–F</strong>, The results from another example neuron. The same conventions as in <strong>A–C</strong>.</font></p><h3 id=fig4-sd-ratios-from-hmax-model-prediction class="relative group">Fig4. SD ratios from HMax model prediction <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#fig4-sd-ratios-from-hmax-model-prediction aria-label=Anchor>#</a></span></h3><p><figure><picture class="mx-auto my-0 rounded-md"><source srcset="/publications/jneurosci_2019/F4.large_hu_db50668cf7e9db9f.webp 330w,/publications/jneurosci_2019/F4.large_hu_8325628f56532bc8.webp 660w
,/publications/jneurosci_2019/F4.large_hu_bebcd66624fbb60c.webp 1024w
,/publications/jneurosci_2019/F4.large_hu_9146b6de1e4cce14.webp 1130w" sizes=100vw type=image/webp><img width=1130 height=1280 class="mx-auto my-0 rounded-md" alt=Fig4 loading=lazy decoding=async src=/publications/jneurosci_2019/F4.large_hu_b614dbfd9eeff60.jpg srcset="/publications/jneurosci_2019/F4.large_hu_76050b15941c703e.jpg 330w,/publications/jneurosci_2019/F4.large_hu_b614dbfd9eeff60.jpg 660w
,/publications/jneurosci_2019/F4.large_hu_4c4688d3db4be474.jpg 1024w
,/publications/jneurosci_2019/F4.large.jpg 1130w" sizes=100vw></picture></figure><font size=2><strong>A–D</strong>, Population results for HMax models optimized based on shape responses only. Model goodness of fit for shape (<strong>A</strong>) and texture (<strong>B</strong>) responses across all neurons. Goodness of fit was determined as the median correlation coefficient (r) of 10-fold cross-validation test sets. Triangles represent median. HMax models provided a good fit for shape responses (median r = 0.61) but a poor fit for the texture responses (median r = 0.02). Predicted response ranges (SD values) for shape and texture stimuli were similar (<strong>C</strong>), and the SD ratios (gray bars in <strong>D</strong>) spanned a narrow range. SD ratios from the observed data (as in Fig. 2D) are overlaid in white (<strong>D</strong>) for comparison. <strong>D</strong>, Light gray bars represent overlap between gray and white distributions. White bars are all the same in <strong>D, H, L</strong>. Yellow shaded area as in Figure 2. E–H, HMax model results optimized based on texture responses only. HMax models provided a poor fit for both shape (median r is 0.07) and texture (median r is 0.23) responses. Model results in terms of SD values and ratios (<strong>G, H</strong>) were similar to those in <strong>C, D</strong>. <strong>I–L</strong>, HMax models optimized simultaneously based on shape and texture responses. Models provided a good fit for shape responses (median r = 0.56) but a poor fit for texture responses (median r = 0.10). In this case, the distribution of SD ratios were similar to V4 data (compare gray and white bars in <strong>L</strong>), but the SDs for predicted texture responses were unlike observed data: note the lack of low (&lt;0.05) and high (>0.2) SDs for texture in K compared with Figure 2C. Asterisk indicate mean value.</font></p><h3 id=fig5-tuning-for-boundary-curvature-in-shape-selective-neurons class="relative group">Fig5. Tuning for boundary curvature in shape-selective neurons <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#fig5-tuning-for-boundary-curvature-in-shape-selective-neurons aria-label=Anchor>#</a></span></h3><p><figure><picture class="mx-auto my-0 rounded-md"><source srcset=/publications/jneurosci_2019/F5.large_hu_4fac6e91c6508e6f.webp sizes=100vw type=image/webp><img width=410 height=1280 class="mx-auto my-0 rounded-md" alt=Fig5 loading=lazy decoding=async src=/publications/jneurosci_2019/F5.large.jpg></picture></figure><font size=2><strong>A</strong>, Shape stimuli that evoked the strongest (preferred) and weakest (nonpreferred) responses from an example neuron (#2; also in Fig. 2). For this neuron, shapes evoked a broader range of responses than textures: SD for shape = 0.23; SD for texture = 0.07. <strong>B</strong>, Responses to shape stimuli were best explained by a 2D Gaussian APC model with a peak at a curvature of −0.27 at 90°, reflecting the preference for concave curvature to the top of the shape. <strong>C</strong>, Responses predicted by the best fit APC model (y axis) are well correlated with the observed responses. <strong>D–F</strong>, Results from a second example neuron (#8). The same conventions as in <strong>A–C</strong>. This neuron responded strongly to shapes with a concave contour at top right of the shape (45°). SD for shape = 0.22; SD for texture = 0.02. <strong>G</strong>, Neurons whose responses are well predicted by the APC model (filled symbols, APC model goodness of fit > 0.6) are identified on a scatter plot of response range for shape and texture stimuli (same as Fig. 2C). This included 42 of 101 neurons across our dataset (right, histogram). Top right, Histogram represents the distribution of SD differences (shape − texture) for highly shape-selective (black bars) and other neurons (white). Mean SD shape minus SD texture was significantly different for the two groups of neurons (Mann–Whitney U test, p &lt; 0.001). Triangles represent median values of the distributions. Gray bars represent overlap between two distributions. Data points corresponding to the example neurons in <strong>A–C</strong> (#2), and <strong>D–F</strong> (#8) are identified. Asterisks indicate that difference between two distributions are statistically significant at the level of p &lt; 0.001 (Mann-Whitney U-test).</font></p><h3 id=fig6-tuning-for-perceptual-dimension-of-texture class="relative group">Fig6. Tuning for perceptual dimension of texture <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#fig6-tuning-for-perceptual-dimension-of-texture aria-label=Anchor>#</a></span></h3><p><figure><picture class="mx-auto my-0 rounded-md"><source srcset="/publications/jneurosci_2019/F6.large_hu_a302ba8653a9c5f.webp 330w,/publications/jneurosci_2019/F6.large_hu_7241e06f662eb41a.webp 660w
,/publications/jneurosci_2019/F6.large_hu_feb09712d99c8c29.webp 1024w
,/publications/jneurosci_2019/F6.large_hu_aef62a175f49aa9c.webp 1280w" sizes=100vw type=image/webp><img width=1280 height=576 class="mx-auto my-0 rounded-md" alt=Fig6 loading=lazy decoding=async src=/publications/jneurosci_2019/F6.large_hu_d2d0f90ce01cd9d0.jpg srcset="/publications/jneurosci_2019/F6.large_hu_2149a20680f60d81.jpg 330w,/publications/jneurosci_2019/F6.large_hu_d2d0f90ce01cd9d0.jpg 660w
,/publications/jneurosci_2019/F6.large_hu_c698985e83f37c98.jpg 1024w
,/publications/jneurosci_2019/F6.large.jpg 1280w" sizes=100vw></picture></figure><font size=2><strong>A</strong>, Texture stimuli that evoked the strongest (preferred) and weakest (nonpreferred) responses from an example neuron (#9). The nonpreferred textures are directional, oriented at different directions, unlike the preferred stimuli, which tend to be nondirectional for this neuron. SD for shape = 0.13; SD for texture = 0.20. <strong>B</strong>, Neural responses for all texture stimuli (y axis) plotted as a function of the directionality index (x axis) shows a statistically significant (p &lt; 0.001) negative correlation. <strong>C, D</strong>, Example neuron (#10) that responded strongly to coarse rather than fine textures. SD for shape = 0.08; SD for texture = 0.11. <strong>E</strong>, Neurons whose responses are well predicted by the texture model (see Materials and Methods; filled symbols, texture model goodness of fit > 0.6) are identified on the scatter plot of response range for shape and texture stimuli. This included 27 of 101 neurons across our dataset (right, histogram). These texture-selective neurons (filled circles) and the other neurons (open circles) showed a significant difference in distribution of shape SD minus texture SD (Mann–Whitney U test, p &lt; 0.001; see top right, histogram). Triangles represent median values of the distributions. There was limited overlap (n = 6) between neurons with APC model goodness of fit > 0.6 and those with texture model goodness of fit > 0.6 (compare filled symbols in Fig. 5G vs Fig. 6E). Data points corresponding to the example neurons in <strong>A</strong> and <strong>B</strong> (#9), and <strong>C</strong> and <strong>D</strong> (#10) are identified.</font></p><h3 id=fig7-joint-coding-of-shape-and-texture class="relative group">Fig7. Joint coding of shape and texture <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#fig7-joint-coding-of-shape-and-texture aria-label=Anchor>#</a></span></h3><p><figure><picture class="mx-auto my-0 rounded-md"><source srcset="/publications/jneurosci_2019/F7.large_hu_b3c9537b03450c93.webp 330w,/publications/jneurosci_2019/F7.large_hu_901d1f857b794728.webp 660w
,/publications/jneurosci_2019/F7.large_hu_9a3e6289ea1d61f.webp 999w
,/publications/jneurosci_2019/F7.large_hu_9a3e6289ea1d61f.webp 999w" sizes=100vw type=image/webp><img width=999 height=1280 class="mx-auto my-0 rounded-md" alt=Fig7 loading=lazy decoding=async src=/publications/jneurosci_2019/F7.large_hu_97b7702084b6fb52.jpg srcset="/publications/jneurosci_2019/F7.large_hu_e9d8047e14dd699d.jpg 330w,/publications/jneurosci_2019/F7.large_hu_97b7702084b6fb52.jpg 660w
,/publications/jneurosci_2019/F7.large.jpg 999w
,/publications/jneurosci_2019/F7.large.jpg 999w" sizes=100vw></picture></figure><font size=2><strong>A</strong>, Responses of an example neuron (#11) to 10 nondirectional textures (x axis) presented through three different shapes apertures (line colors). Responses to the three shapes presented with a uniform luminance contrast are also shown (leftmost symbols) for comparison. Error bars indicate ±1 SEM. This neuron exhibited a broader range of shape responses than texture responses (SD for shape = 0.19, SD for texture = 0.10), but overall, shape preference was largely preserved across textures. <strong>B</strong>, Example neuron (#12) with a strong preference for texture but not shape (SD for shape = 0.07, SD for texture = 0.16). All details as in <strong>A</strong>. <strong>C</strong>, Neuron 8 showed selectivity along shape and texture dimensions. Preference for fine textures was observable only within the preferred shape boundary. <strong>D–F</strong>. Additional example neurons (#13, #14, #15) that exhibited joint tuning for shape and texture. <strong>G</strong>, Effect size (see Materials and Methods) of texture was compared with that of shape for each of 43 neurons subjected to the control experiment. Data points corresponding to the example neurons (<strong>A–F</strong>) are identified. <strong>H</strong>, To quantify the independence of shape and texture tuning, we evaluated whether responses to shape-texture combination stimuli can be predicted by the product of the responses to shape and texture. Scatter plots show observed responses versus those predicted by a multiplicative model (see Materials and Methods) for neurons in <strong>A–F</strong>. <strong>I</strong>, Comparison between multiplicative (x axis) model and additive (y axis) model. Goodness-of-fit (r) values were quantified by the correlation coefficient between observed and predicted responses across all neurons (n = 43). Multiplicative model (median r = 0.91) generally provides a better fit than an additive model (median r = 0.86). Asterisk indicate median value.</font></p><h3 id=fig8-temporal-dynamics-of-shape-and-texture-selectivity class="relative group">Fig8. Temporal dynamics of shape and texture selectivity <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#fig8-temporal-dynamics-of-shape-and-texture-selectivity aria-label=Anchor>#</a></span></h3><p><figure><picture class="mx-auto my-0 rounded-md"><source srcset="/publications/jneurosci_2019/F8.large_hu_616dbc81e0030055.webp 330w,/publications/jneurosci_2019/F8.large_hu_29db07263ac7dd42.webp 660w
,/publications/jneurosci_2019/F8.large_hu_d3d900fa8461d70f.webp 1024w
,/publications/jneurosci_2019/F8.large_hu_934281e6b15827dc.webp 1280w" sizes=100vw type=image/webp><img width=1280 height=1268 class="mx-auto my-0 rounded-md" alt=Fig8 loading=lazy decoding=async src=/publications/jneurosci_2019/F8.large_hu_ca8f4d7eae6fed7d.jpg srcset="/publications/jneurosci_2019/F8.large_hu_12fa67c6ee9bef16.jpg 330w,/publications/jneurosci_2019/F8.large_hu_ca8f4d7eae6fed7d.jpg 660w
,/publications/jneurosci_2019/F8.large_hu_3d407daf261d846.jpg 1024w
,/publications/jneurosci_2019/F8.large.jpg 1280w" sizes=100vw></picture></figure><font size=2><strong>A</strong>, PSTHs for shape (left), large and small aperture textures (middle and right, respectively) are shown for preferred (red; top 50% of stimuli based on spike counts between 50 and 400 ms after stimulus onset; see Materials and Methods), nonpreferred (blue; bottom 50%), and all stimuli (black) for one example neuron. Shaded area represents ±1 SEM. For shape stimuli, difference in responses between preferred and nonpreferred stimuli emerged soon (50 ms) after response onset. For textures, statistically significant difference emerged at 100 ms after stimulus onset. <strong>B</strong>, Second example neuron showing delayed emergence of texture selectivity (shape-dependent modulation ≥ 51 ms; texture-dependent modulation ≥ 88 ms; earlier onset was determined for small aperture texture condition). <strong>C</strong>, Across all neurons, onset of shape selectivity (x axis) is plotted against onset of texture selectivity (y axis). Filled and open symbols represent large and small aperture conditions, respectively. Data points from the same neuron are connected by a vertical line. In a few neurons (data points without vertical line), onset of texture selectivity could not be defined for one of the aperture conditions due to weak responses. Most data points lie above the diagonal line, indicating that texture information is processed later than shape information. <strong>D</strong>, Marginal histograms for onset times for shape (gray), large aperture texture (black), and small aperture texture (white). Triangles represent the mean of each distribution (shape = 55.72 ms, large aperture texture = 85.53 ms, small aperture texture = 85.78 ms). On average, onset of shape selectivity was ∼30 ms faster than onset of texture selectivity.</font></p></div></section><footer class="max-w-prose pt-8 print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="group flex" href=/publications/ejn_2017/><span class="me-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&larr;</span><span class="ltr:hidden rtl:inline">&rarr;</span></span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Binocular function during unequal monocular input</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2017-02-01 00:00:00 +0000 UTC">1 February 2017</time>
</span></span></a></span><span><a class="group flex text-right" href=/publications/curropinneurobiol_2019/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Object shape and surface properties are jointly encoded in mid-level ventral visual cortex</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2019-10-01 00:00:00 +0000 UTC">1 October 2019</time>
</span></span><span class="ms-2 text-neutral-700 transition-transform group-hover:-translate-x-[-2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&rarr;</span><span class="ltr:hidden rtl:inline">&larr;</span></span></a></span></div></div></footer></article></main><div class="pointer-events-none absolute bottom-0 end-0 top-[100vh] w-12" id=to-top hidden=true><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div><footer class="py-10 print:hidden"><div class="flex items-center justify-between"><div><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Taekjun Kim</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://github.com/jpanther/congo target=_blank rel="noopener noreferrer">Congo</a></p></div><div class="flex flex-row items-center"></div></div></footer><div id=search-wrapper class="invisible fixed inset-0 z-50 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://taekjunkim.github.io/><div id=search-modal class="top-20 mx-auto flex min-h-0 w-full max-w-3xl flex-col rounded-md border border-neutral-200 bg-neutral shadow-lg dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex flex-none items-center justify-between px-2"><form class="flex min-w-0 flex-auto items-center"><div class="flex h-8 w-8 items-center justify-center text-neutral-400"><span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="mx-1 flex h-12 flex-auto appearance-none bg-transparent focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex h-8 w-8 items-center justify-center text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto overflow-auto px-2"><ul id=search-results></ul></section></div></div></div></body></html>