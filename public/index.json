[{"content":" Taekjun Kim Neuroscientist | University of Washington Welcome to my website! üéâ I am Taekjun Kim, a computational neuroscientist at the University of Washington and the Washington National Primate Research Center. I study how neural circuits in the brain process sensory information from the environment and guide appropriate behaviors. Specifically, my research focuses on understanding the neural mechanisms that enable visual object recognition and scene perception in the ventral visual pathway of macaque monkeys.\nMy work combines neurophysiology experiments, behavioral studies, and computational modeling. I use advanced techniques like high-density electrophysiology and Ca++ imaging to record neural activity across multiple brain regions. By integrating experimental data with computational approaches, I seek to extract meaningful insights from the complex datasets.\n","date":null,"permalink":"/","section":"","summary":"","title":""},{"content":"","date":null,"permalink":"/tags/information/","section":"Tags","summary":"","title":"Information"},{"content":"","date":null,"permalink":"/tags/knowledge/","section":"Tags","summary":"","title":"Knowledge"},{"content":"","date":null,"permalink":"/tags/note/","section":"Tags","summary":"","title":"Note"},{"content":"General information #The PARA method is a system for organizing and managing knowledge, information, and tasks in a way that maximizes efficiency and clarity. Developed by Tiago Forte, the PARA method stands for Projects, Areas, Resources, and Archives. It‚Äôs designed to help individuals and teams manage their digital information and workflows, ensuring that everything is easy to access and align with broader goals. The Four Components of the PARA Method:\nProjects:\nThese are active tasks or goals that you\u0026rsquo;re working on. Projects are typically time-bound, with specific outcomes and deadlines. Example: \u0026ldquo;Write a research paper,\u0026rdquo; or \u0026ldquo;Design a new website.\u0026rdquo; Areas:\nThese are ongoing responsibilities or areas of focus that don‚Äôt have a specific deadline. They are continuous and require regular attention, but they are not tied to any single project. Example: \u0026ldquo;Family health\u0026rdquo;, \u0026ldquo;Home management\u0026rdquo; Resources:\nThese are collections of useful materials or reference information that support your projects and areas. Resources are essentially the knowledge or tools you reference to help you with work, but they aren\u0026rsquo;t tasks themselves. Example: \u0026ldquo;Research articles on machine learning\u0026rdquo;, \u0026ldquo;Templates for presentations\u0026rdquo; Archives:\nInformation that is no longer active but needs to be preserved for future reference or historical purposes. Example: \u0026ldquo;Finished reports\u0026rdquo;, \u0026ldquo;Past meeting notes\u0026rdquo; By using PARA, you have a clear structure for storing, organizing, and accessing your knowledge and tasks. As your projects or responsibilities evolve, the system allows you to adapt and refine your workflow, ensuring you stay organized and productive.\n","date":null,"permalink":"/posts/para/","section":"Posts","summary":"how to use PARA method for knowledge management","title":"PARA method for knowledge management"},{"content":" Things I find interesting ","date":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"/tags/area-v4/","section":"Tags","summary":"","title":"Area V4"},{"content":"","date":null,"permalink":"/tags/crowding/","section":"Tags","summary":"","title":"Crowding"},{"content":"","date":null,"permalink":"/tags/electrophysiology/","section":"Tags","summary":"","title":"Electrophysiology"},{"content":"Article info # Authors Taekjun Kim, Anitha Pasupathy Publication date 2024/06/12 Journal Journal of Neuroscience DOI https://doi.org/10.1523/JNEUROSCI.2260-23.2024 Abstract #Visual crowding refers to the phenomenon where a target object that is easily identifiable in isolation becomes difficult to recognize when surrounded by other stimuli (distractors). Many psychophysical studies have investigated this phenomenon and proposed alternative models for the underlying mechanisms. One prominent hypothesis, albeit with mixed psychophysical support, posits that crowding arises from the loss of information due to pooled encoding of features from target and distractor stimuli in the early stages of cortical visual processing. However, neurophysiological studies have not rigorously tested this hypothesis. We studied the responses of single neurons in macaque (one male, one female) area V4, an intermediate stage of the object-processing pathway, to parametrically designed crowded displays and texture statistics-matched metameric counterparts. Our investigations reveal striking parallels between how crowding parameters‚Äînumber, distance, and position of distractors‚Äîinfluence human psychophysical performance and V4 shape selectivity. Importantly, we also found that enhancing the salience of a target stimulus could alleviate crowding effects in highly cluttered scenes, and this could be temporally protracted reflecting a dynamical process. Thus, a pooled encoding of nearby stimuli cannot explain the observed responses, and we propose an alternative model where V4 neurons preferentially encode salient stimuli in crowded displays. Overall, we conclude that the magnitude of crowding effects is determined not just by the number of distractors and target‚Äìdistractor separation but also by the relative salience of targets versus distractors based on their feature attributes‚Äîthe similarity of distractors and the contrast between target and distractor stimuli.\nFigures #Fig1. Visual stimulus design # A, Tabulation of clutter conditions. For each target‚Äìdistractor configuration (rows), responses to eight target rotations (# Conditions‚Äâ=‚Äâ8) were evaluated, except when distractors were presented without a target (Target‚Äâ=‚ÄâNone). An open circle under Metamer column indicates that metameric stimuli were also presented (see Materials and Methods). B, Shape stimulus set. Target (red box) and distractor shapes were chosen from a set of 2D shapes [a subset of shapes created by Pasupathy and Connor (2001)]. Target stimulus was at the RF center, scaled to half of the estimated RF diameter. C, The target stimulus was presented either (i) alone or in combination with various distractor arrangements which varied in terms of (ii) distance from central target, (iii) number, (iv) saliency defined by target color, and (v) shape, shape‚Äâ+‚Äâsize of distractors. In all conditions, the target shape was shown at eight rotations in 45¬∞ increments (as in i). Targets were achromatic or chromatic when presented alone. In all clutter conditions targets were achromatic except when target saliency was titrated by color (iv). Distractors were always achromatic. The target size was the same in all conditions, but it is scaled down in ii for illustration purposes. Distractors were the same size as target except when titrating saliency by size (yellow dot). Metameric stimulus pairs with matched texture statistics (in vi: panels 1‚Äì2, 3‚Äì4, and 5‚Äì6) were included to test the texture-pooling model (see Materials and Methods). Colored dots in A and C identify identical stimulus conditions repeated in the figure for illustration purposes. Fig2. Effect of target‚Äìdistractor distance on shape tuning # A‚ÄìC, Example neuron. A, Raster plots and PSTHs for responses to (rows) target alone, target‚Äâ+‚Äâfar, middle, and near distractor conditions, respectively. Columns show responses to different target orientations, rank-ordered by responses to target-alone. Stimulus panels are shown here at a higher contrast to aid visibility (see Fig. 1C for veridical illustration). B, Tuning curves based on average responses (0‚Äì400‚ÄÖms) for the four target‚Äâ+‚Äâdistractor conditions. Error bars indicate the standard error of the mean. C, Average PSTHs for the preferred (top 4) and nonpreferred (bottom 4) targets for each distractor condition are shown in solid and dotted lines, respectively. Black asterisks indicate time points with significant difference between solid and dotted curves (Mann‚ÄìWhitney U test in a 30‚ÄÖms sliding window; p‚Äâ\u0026lt;‚Äâ0.05). D‚ÄìF, Population results. D, Average normalized tuning curves across the subpopulation of neurons with significant shape selectivity in the target alone condition (73/147). Error bars indicate the standard error of the mean. E, The distribution of distractor modulation index (DMI) for each distractor condition is presented. Filled bars represent neurons whose shape tuning curve exhibited statistically significant correlations with that in the ‚Äútarget alone‚Äù condition. Red symbols denote the mean correlation values in neurons within the lower (0‚Äì50%), middle (25‚Äì75%), and upper (50‚Äì100%) ranges of the DMI distribution. F, The proportion of neurons with significant shape-dependent modulation (i.e., asterisks in C) as a function of time for each distractor condition. The red shaded area, representing the equivalent analysis for the ‚Äútarget alone‚Äù condition, is included for panel-to-panel comparison. Fig3. Effect of distractor number on responses and selectivity # A‚ÄìC, Example neuron responses. A, Raster plots with PSTHs for responses to (rows) target alone, target‚Äâ+‚Äâ1, 3, or 6 distractors, respectively. Columns show responses to different target orientations. B, Target shape selectivity curves of the example unit for the four different conditions. C, Average PSTHs for the preferred (top 4) and nonpreferred (bottom 4) targets. D‚ÄìF, Population results. D, Average normalized tuning curves for target shape selectivity in the presence and absence of distractors. E, The distribution of distractor modulation index (DMI) for each distractor condition is presented. Filled bars represent neurons whose shape tuning curve exhibited statistically significant correlations with that in the ‚Äútarget alone‚Äù condition. Red symbols denote the mean correlation values in neurons within the lower (0‚Äì50%), middle (25‚Äì75%), and upper (50‚Äì100%) ranges of the DMI distribution. F, The proportion of neurons with significant shape-dependent modulation (i.e., asterisks in C) as a function of time for each distractor condition. All conventions are as in Figure 2. Fig4. Effects of distractor position on visual crowding: anisotropic crowding zone # A, B, RF summary illustrates both location (A) and size (B). RFs from two monkeys are positioned in the lower right visual field and their sizes increase with eccentricity. Red line in B denotes the values computed using the equation employed to estimate the RF diameter (see Materials and Methods). C, For each neuron, the positions of the distractors were transformed into a radial/tangential axis relative to the fixation point. The yellow circles depict two example RF locations. The red, green, and blue arrows indicate the radial outward, radial inward, and tangential directions with respect to the target location. D, The histograms illustrate the correlation between tuning curves for target alone versus target‚Äâ+‚Äâone distractor calculated at 12 distractor positions, which were realigned based on the radial and tangential axes originating from the fixation point. Filled bars indicate statistically significant cases. In the polar plot, red and black data points compare the median values (red vertical lines) and proportion of significant cases (filled bars) from histograms of the matched directions, respectively. E, RF shape estimation from four example neurons. A 2D Gaussian fit (white ellipses) is superimposed on the raw response map (7‚Äâ√ó‚Äâ7 grid, 1¬∞ intervals). Theta angle represents the counterclockwise angle of the major axis of RF with respect to the horizontal line. Red and blue dots indicate Gaussian fit center and RF hotspot, respectively. F, Distribution of the major axis orientation in the Gaussian RF fitting of the recorded neurons. Red vertical line indicates the median value. For the RFs in the lower right visual field, an angle bigger than 90¬∞ represents an elongated RF shape oriented toward the fixation point. G, In most neurons, the RF hotspot (y-axis) is closer to the fixation point than the center of the 2D Gaussian fit (Wilcoxon signed-rank test; p‚Äâ\u0026lt;‚Äâ0.05), suggesting that the RF extent is larger in the outward radial direction compared with the inward direction. The red line represents the unity line. Fig5. Effects of target saliency (color cue) on visual crowding # Target (gray or colored) appeared alone or in combination with six random distractors. A‚ÄìC, Example unit responses. A, Raster plots with PSTHs. B, Target shape selectivity curves of the example unit for the four different conditions. C, Average PSTHs for the preferred (top 4) and nonpreferred (bottom 4) targets. D‚ÄìF, Population results. D, Average normalized tuning curves for target shape selectivity across conditions. E, The distribution of distractor modulation index (DMI) for each distractor condition. Filled bars represent neurons whose shape tuning curve exhibited statistically significant correlation with that in the ‚Äútarget alone‚Äù condition. Red symbols denote the mean correlation values for neurons within the lower (0‚Äì50%), middle (25‚Äì75%), and upper (50‚Äì100%) ranges of the DMI distribution. F, The proportion of neurons with significant shape-dependent modulation as a function of time for each distractor condition. All conventions are as in Figure 2. Fig6. Effects of target saliency (shape, size cues) on visual crowding # Target appeared alone or in combination with 12 small circles, six circles, or six random distractors. A‚ÄìC, Example unit responses. A, Raster plots with PSTHs. B, Target shape selectivity curves of the example unit from four different conditions. C, Average PSTHs for the preferred (top 4) and nonpreferred (bottom 4) targets. D‚ÄìF, Population results. D, Average normalized tuning curves for target shape selectivity across conditions. E, The distribution of distractor modulation index (DMI) for each distractor condition. Filled bars represent neurons whose shape tuning curve exhibited statistically significant correlation with that in the ‚Äútarget alone‚Äù condition. Red symbols denote the mean correlation values for neurons within the lower (0‚Äì50%), middle (25‚Äì75%), and upper (50‚Äì100%) ranges of the DMI distribution. F, The proportion of neurons with significant shape-dependent modulation as a function of time for each distractor condition. All conventions are as in Figure 2. Fig7. Test of texture statistics model # A‚ÄìC, Test of the texture statistics model for visual crowding. A, For each neuron, we computed two correlations: (1) the correlation between responses to ‚Äútarget‚Äâ+‚Äârandom distractor‚Äù and ‚Äútarget alone‚Äù stimuli and (2) the correlation between responses to ‚Äútarget‚Äâ+‚Äârandom distractor‚Äù stimuli and matched metamers. Population data are shifted below the diagonal suggesting that responses to target‚Äâ+‚Äârandom distractors are better correlated with the ‚Äútarget alone‚Äù condition (x-axis). Cells with a significant correlation (p‚Äâ\u0026lt;‚Äâ0.05) in x-axis alone, y-axis alone, or both are identified (see legend). B, C, The same analyses as in A for the ‚Äúcircle distractor‚Äù (B) and ‚Äúsmall circle distractor‚Äù (C) conditions in which the target is more salient. Fig8. Temporal dynamics of population decoding # Population decoding performance plotted as a function of time across different distractor conditions. In all four panels, target alone (red) and target‚Äâ+‚Äâ6 near distractors (black) are identical. Decoding performance declines in the presence of distractors but time course varies across conditions. For the salient target conditions (light blue curve in C, gray curves in D), rise time and the maximum decoding performance time are delayed compared with target alone conditions (red and blue curves in C,D), but this is not the case for distance and number effects (gray curves in A,B). Green line indicates the chance level of target orientation decoding (0.125, 1 out of 8). Different colored lines represent different target‚Äìdistractor configurations. Fig9. Hierarchical saliency computation model # Visual input is first processed in parallel by a set of low-level feature detectors (e.g., orientation, color, luminance, texture) in earlier visual areas. To focus on the central region of the visual scene, visual inputs (112‚Äâ√ó‚Äâ112 pixels) and feature maps (28‚Äâ√ó‚Äâ28 pixels) were cropped from the larger images of size 224‚Äâ√ó‚Äâ224 pixels and 55‚ÄÖ√ó‚Äâ55 pixels, respectively. Feature maps show the outputs from the first five filters from AlexNet. The next stage of processing performs a RF center-surround operation for each feature dimension and selectively combines only informative feature maps in which the RF center region is more strongly activated than its surround (see more details in the main text). ","date":"12 June 2024","permalink":"/publications/jneurosci_2024/","section":"Selected publications","summary":"Taekjun Kim, Anitha Pasupathy \u003ccode\u003eJournal of Neuroscience\u003c/code\u003e","title":"Neural correlates of crowding in macaque area V4"},{"content":"","date":null,"permalink":"/tags/neuroscience/","section":"Tags","summary":"","title":"Neuroscience"},{"content":"","date":null,"permalink":"/tags/primate/","section":"Tags","summary":"","title":"Primate"},{"content":" Peer-reviewed academic papers that I published ","date":null,"permalink":"/publications/","section":"Selected publications","summary":"","title":"Selected publications"},{"content":"","date":null,"permalink":"/tags/vision/","section":"Tags","summary":"","title":"Vision"},{"content":"","date":null,"permalink":"/tags/computer-vision/","section":"Tags","summary":"","title":"Computer Vision"},{"content":" \u003c!DOCTYPE html\u003e FaceMorphing I implemented face morphing using opencv and scipy. This jupyter notebook demonstrates the \"Face Morphing Procedures\" step by step.\nIt‚Äôs inspired by the information provided in the link below:\nhttps://github.com/Azmarie/Face-Morphing/\nimport modules¬∂ In¬†[1]: import numpy as np # data manipulation import matplotlib.pyplot as plt # plotting from matplotlib.patches import Rectangle # plotting import cv2 # computer vision package import dlib # to use a pre-trained model to detect face landmarks import time from IPython import display # for update plot in a loop from scipy.spatial import Delaunay, Voronoi from tqdm import tqdm; 1. Prepare two photos with a front face¬∂ In¬†[7]: ### Read in the source and target images imgA = cv2.imread('./EmmaWatson1.jpg') imgB = cv2.imread('./EmmaWatson2.png') ### face detection def detect_face(img, ax_num): face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'); img_faces = face_cascade.detectMultiScale(img, 1.1, 5); plt.subplot(1, 2, ax_num); plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)); for (x,y,w,h) in img_faces: rect = Rectangle((x,y),w,h, linewidth=2, edgecolor='r', facecolor='none'); plt.gca().add_patch(rect); return img_faces; imgA_face = detect_face(imgA, 1); imgB_face = detect_face(imgB, 2); 2. Resize and crop the image¬∂ Cropped image size: 700 x 700 Face region: 500 x 500 at the center In¬†[8]: ### resize and crop around the face # resize an image so that the face region becomes 500px x 500px # crop the image with 100px margin: cropped image size becomes 700px x 700px def resize_crop(img, face, ax_num): h, w, c = img.shape; scale_factor = 500/np.mean(face[0][2:]); new_w = int(w*scale_factor); new_h = int(h*scale_factor); dsize = (new_w, new_h); new_face = face[0]*scale_factor; print(face, new_face); resized = cv2.resize(img, dsize); anchor_x = int(new_face[0])-100; anchor_y = int(new_face[1])-100; cropped = resized[anchor_y:anchor_y+700, anchor_x:anchor_x+700]; plt.subplot(1, 2, ax_num); plt.imshow(cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB)); return cropped; imgA = resize_crop(imgA, imgA_face, 1); imgB = resize_crop(imgB, imgB_face, 2); [[105 51 184 184]] [285.32608696 138.58695652 500. 500. ] [[ 46 73 153 153]] [150.32679739 238.5620915 500. 500. ] 3. Find point correspondences using facial feature detection¬∂ In¬†[9]: ### find face landmarks # use dlib library # use pretrained model: shape_predictor_68_face_landmarks.dat detector = dlib.get_frontal_face_detector(); predictor = dlib.shape_predictor(\"./shape_predictor_68_face_landmarks.dat\"); def get_landmarks(img, ax_num): img_gray = cv2.cvtColor(src=img, code=cv2.COLOR_BGR2GRAY); faces = detector(img_gray) landmarks = predictor(image=img_gray, box=faces[0]) plt.subplot(1,2,ax_num) plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)); landmarks_points = []; for n in range(0, 68): x = landmarks.part(n).x y = landmarks.part(n).y landmarks_points.append([x,y]); # Draw a circle plt.plot(x, y, 'o', color=(0, 1, 0)) # add points at corners landmarks_points.append([0,0]); landmarks_points.append([0,699]); landmarks_points.append([699,0]); landmarks_points.append([699,699]); return landmarks_points; imgA_landmarks = get_landmarks(imgA, 1); imgB_landmarks = get_landmarks(imgB, 2); 4. Get area correspondences using triangulation¬∂ From the previous step we have two sets of 72 (68(face) + 4(corner)) points ‚Äî one set per image. On these points, we performed Delaunay triangulation and it produces 138 triangles connecting the 72 points In¬†[10]: ### triangulation # from scipy.spatial import Delaunay def plot_triangulation(img, points, ax_num): points = np.array(points); triangulation = Delaunay(points); plt.subplot(1,2,ax_num); v = Voronoi(points) plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) plt.triplot(points[:,0], points[:,1], triangulation.simplices.copy(), c='w') plt.plot(points[:,0], points[:,1], 'wo') return None; plot_triangulation(imgA, imgA_landmarks, 1); plot_triangulation(imgB, imgB_landmarks, 2); 5. Warping images and alpha blending¬∂ Calculate the location ($x_m, y_m$) of the pixel in the morphed image, imgM\nWeighted average of 72 Point Correspondences from imgA and imgB $x_m = (1-\\alpha)x_A + \\alpha x_B$ $y_m = (1-\\alpha)y_A + \\alpha y_B$ Calculate affine transforms\nWe now have 72 corresponding points from imgA, imgB, and imgM We now have 138 corresponding triangles from imgA, imgB, and imgM Pick a triangle in imgA and the corresponding triangle in imgM Calculate the affine transform that maps the three corners of the triangle in imgA to the three corners of the corresponding triangle in imgM Calculate an affine transform for every pair of 138 triangles. Repeat the process of imgB and imgM. Warp triangle\nFor each triangle in imgA, use the affine transform calculated in the previous step to transform all pixels inside the triangle to imgM. Repeat this for all triangles in imgA to obtain a warped version of imgA. Similarly, obtain a warped version for imgB. Alpha blend warped images\nIn the previous step we obtained warped version of imgA and imgB. These two images can be alpha blended using equation $M(x_m, y_m) = (1-\\alpha)A(x_a, y_a) + \\alpha B(x_b, y_b)$ In¬†[11]: alpha_list = np.arange(0, 1.02, 0.02); imgM_list = []; for i in tqdm(range(len(alpha_list))): # alpha is changing alpha = alpha_list[i]; # output image imgM = np.zeros(imgA.shape, dtype = imgA.dtype) # Calculate the landmark points in the morphed image, imgM imgM_landmarks = (1-alpha)*np.array(imgA_landmarks, dtype=float) + \\ alpha*np.array(imgB_landmarks, dtype=float); # triangulation in imgM imgM_tri = Delaunay(imgM_landmarks); for t in range(len(imgM_tri.simplices)): # which point# are in this triangle simplices_now = imgM_tri.simplices[t]; # x,y values for this triangle in imgA, imgB, imgM imgA_points = []; imgB_points = []; imgM_points = []; for p in range(3): x = imgA_landmarks[simplices_now[p]][0]; y = imgA_landmarks[simplices_now[p]][1]; imgA_points.append([x,y]); x = imgB_landmarks[simplices_now[p]][0]; y = imgB_landmarks[simplices_now[p]][1]; imgB_points.append([x,y]); x = imgM_landmarks[simplices_now[p]][0]; y = imgM_landmarks[simplices_now[p]][1]; imgM_points.append([x,y]); # Find bounding rectangle for each triangle rectA = cv2.boundingRect(np.float32([imgA_points])) rectB = cv2.boundingRect(np.float32([imgB_points])) rectM = cv2.boundingRect(np.float32([imgM_points])) # Offset points by left top corner of the respective rectangles from_rectA = [] from_rectB = [] from_rectM = [] for i in range(0, 3): from_rectM.append(((imgM_points[i][0] - rectM[0]),(imgM_points[i][1] - rectM[1]))); from_rectA.append(((imgA_points[i][0] - rectA[0]),(imgA_points[i][1] - rectA[1]))); from_rectB.append(((imgB_points[i][0] - rectB[0]),(imgB_points[i][1] - rectB[1]))); # Get mask by filling triangle mask = np.zeros((rectM[3], rectM[2], 3), dtype = np.float32) cv2.fillConvexPoly(mask, np.int32(from_rectM), (1.0, 1.0, 1.0), 16, 0) # small rectangular patches of imgA, imgB imgA_patch = imgA[rectA[1]:rectA[1] + rectA[3], rectA[0]:rectA[0] + rectA[2]]; imgB_patch = imgB[rectB[1]:rectB[1] + rectB[3], rectB[0]:rectB[0] + rectB[2]]; # Given a pair of triangles, find the affine transform. warpMat_A = cv2.getAffineTransform( np.float32(from_rectA), np.float32(from_rectM) ) warpMat_B = cv2.getAffineTransform( np.float32(from_rectB), np.float32(from_rectM) ) # Apply the Affine Transform just found to the src image size = (rectM[2], rectM[3]); warpedA_patch = cv2.warpAffine( imgA_patch, warpMat_A, (size[0], size[1]), None, flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101 ) warpedB_patch = cv2.warpAffine( imgB_patch, warpMat_B, (size[0], size[1]), None, flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101 ) # Alpha blend rectangular patches warped_patch = (1.0 - alpha) * warpedA_patch + alpha * warpedB_patch; # Copy triangular region of the rectangular warped patch to the output image imgM[rectM[1]:rectM[1]+rectM[3], rectM[0]:rectM[0]+rectM[2]] = \\ imgM[rectM[1]:rectM[1]+rectM[3], rectM[0]:rectM[0]+rectM[2]] * ( 1 - mask ) + warped_patch * mask imgNow = cv2.resize(imgM, (250,250)); imgM_list.append(cv2.cvtColor(imgNow, cv2.COLOR_BGR2RGB)); 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:06\u0026lt;00:00, 7.32it/s] In¬†[16]: import imageio imageio.mimsave('./EmmaWatson.gif', imgM_list, duration = 0.1, loop=0); In¬†[17]: from IPython.display import Image Image(open('./EmmaWatson.gif','rb').read()) Out[17]: ","date":null,"permalink":"/posts/face_morph/","section":"Posts","summary":"Face morphing implementation in Python","title":"Face Morphing"},{"content":"","date":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python"},{"content":"Article info # Authors Anthony Bigelow*, Taekjun Kim*, Tomoyuki Namima, Wyeth Bair, Anitha Pasupathy Publication date 2023/02/03 Journal Current Biology DOI https://doi.org/10.1016/j.cub.2023.01.016 * These authors contributed equally\nAbstract #A paradox exists in our understanding of motion processing in the primate visual system: neurons in the dorsal motion processing stream often strikingly fail to encode long-range and perceptually salient jumps of a moving stimulus. Psychophysical studies suggest that such long-range motion, which requires integration over more distant parts of the visual field, may be based on higher-order motion processing mechanisms that rely on feature or object tracking. Here, we demonstrate that ventral visual area V4, long recognized as critical for processing static scenes, includes neurons that maintain direction selectivity for long-range motion, even when conflicting local motion is present. These V4 neurons exhibit specific selectivity for the motion of objects, i.e., targets with defined boundaries, rather than the motion of surfaces behind apertures, and are selective for direction of motion over a broad range of spatial displacements and defined by a variety of features. Motion direction at a range of speeds can be accurately decoded on single trials from the activity of just a few V4 neurons. Thus, our results identify a novel motion computation in the ventral stream that is strikingly different from, and complementary to, the well-established system in the dorsal stream, and they support the hypothesis that the ventral stream system interacts with the dorsal stream to achieve the higher level of abstraction critical for tracking dynamic objects.\nFigures #Fig1. V4 responses to long-range motion stimuli # A, Stimulus design. An elliptical patch was displayed in sequence at seven locations centered on the RF (white circle, shown for illustration only). At each location, the stimulus was presented for 100 ms before disappearing and reappearing at the next location instantaneously. Spatial displacements were scaled to a third of the estimated RF diameter. Eight directions were tested; opposite directions sampled identical locations.\nB, Mean responses and SEM of five example neurons (rows) to long-range motion (LRM_noise and LRM_sinusoid) and drifting sinusoidal gratings (local). When LRM was combined with local motion in the same or opposite directions (LRM_sinusoid + local), tuning for LRM was similar. Dashed lines: baseline. C, Population data. Histograms of DI values for LRM and local stimuli. Black/gray bars (and corresponding numbers) show neurons with/without statistically significant modulation by motion direction (one-way ANOVA, p \u0026lt; 0.05). Arrows: median DI across the dataset. Numbers in parentheses show neurons with DI ‚â• 0.5 and statistically significant modulation. Fig2. Comparison of MT and V4 data # A, Size of the spatial displacement (dX) is plotted against DILRM_noise for each of the neurons in Figure 1. Filled symbols denote neurons that exhibit responses significantly modulated by motion direction (one-way ANOVA, p \u0026lt; 0.05). The shaded area represents moderate direction selectivity (DI ‚â• 0.5). Red arrow denotes the range (0.25¬∞‚Äì1¬∞) of maximum dX values in V1 and MT. B, Local dominance histograms for V4 data (top) from this study versus MT data (bottom), reproduced with permission from Hedges et al. DI values for LRM_sinusoid and local were calculated as in Hedges et al. For MT, local dominance is centered around 1, suggesting much stronger selectivity for local than LRM, unlike for V4 data. Fig3. Direction selectivity for object motion across spatiotemporal scales # A, Schematic of the Neuropixels probe showing a subset of the 384 contact pairs (black and white squares) along the shank. B, Waveforms of six example neurons (colors) detected at multiple nearby contacts. Numbers indicate the contact associated with the largest amplitude waveforms. C, Mean responses of example neurons as a function of motion direction for longer range (gray) and shorter range (white) object motion, and local gratings (yellow). Corresponding spatial step sizes (dX) are shown. Error bars: SEM. D, Distribution of DI values. Black/gray bars (and n) show neurons with/without statistically significant modulation of responses by motion direction (one-way ANOVA, p \u0026lt; 0.05). arrows: median DI across the dataset. Numbers in parentheses show neurons with DI ‚â• 0.5 and statistically significant modulation. Fig4. V4 direction sensitivity for object versus surface motion and chromatic boundaries # A, Direction tuning curves for an example neuron to object (left) and surface (right) motion at three speeds. Error bars: SEM. Dashed lines: baseline activity. B, Population histograms of the correlation between tuning curves for object (left column)/surface (right column) motion versus those of other motion types and speeds (rows). Triangles denote median values. Filled bars identify neurons with statistically significant positive correlation (p \u0026lt; 0.05). Correlations within the same motion type (object √ó object or surface √ó surface) were higher than correlation between object and surface motion curves at all speeds. C, Direction tuning curves for two example neurons (columns) measured with translating chromatic and achromatic bars (rows) at five luminance contrasts (see legend). Direction selectivity is evident for chromatic bars and noise patches (bottom row) but not achromatic bars. Fig5. Position invariance of motion direction selectivity # Responses of an example V4 neuron to object motion stimuli at five positions within the RF. Error bars: SEM. Dashed lines: baseline activity. At each position, object motion was created with three sequential stimulus patches with dX = RFd/6, dT = 50 ms in eight directions. The radial distance between the center stimulus location and the other locations was 1/3 √ó RFd. This neuron responded strongly at three of the five spatial positions. In all positions, the direction tuning curve was highly consistent and correlation in tuning was 0.68 (STAR Methods). Across 31 neurons studied with this paradigm, 11 exhibited statistically significant direction tuning at two or more locations. For these neurons, the mean aggregate correlation (STAR Methods) in direction tuning across locations was 0.61. Fig6. Population decoding of object motion direction # A, Low-dimensional representation (first two PCs) of the activity of the direction-selective sub-population of neurons for different object motion directions (colors) for speed = 18.2¬∞/s; this yields a systematic, robust space for deciphering motion direction. B, The same analysis as in (A), based on the activity of non-direction-selective neurons, produces weak/non-existent patterning. C, Projection of data for other speeds from directional neurons onto the space derived in (A). Percent correct performance of a binary decision tree for decoding the object motion direction exactly (C) or within 45¬∞ (C45) is reported for each speed. ","date":"3 February 2023","permalink":"/publications/currbio_2023/","section":"Selected publications","summary":"Anthony Bigelow*, Taekjun Kim*, Tomoyuki Namima, Wyeth Bair, Anitha Pasupathy \u003ccode\u003eCurrent Biology\u003c/code\u003e","title":"Dissociation in neuronal encoding of object versus surface motion in the primate brain"},{"content":"","date":null,"permalink":"/tags/motion/","section":"Tags","summary":"","title":"Motion"},{"content":"","date":null,"permalink":"/tags/perception/","section":"Tags","summary":"","title":"Perception"},{"content":"Article info # Authors Taekjun Kim, Wyeth Bair, Anitha Pasupathy Publication date 2022/01/26 Journal Journal of Neuroscience DOI https://doi.org/10.1523/JNEUROSCI.0971-21.2021 Abstract #Texture is an important visual attribute for surface pattern discrimination and therefore object segmentation, but the neural bases of texture perception are largely unknown. Previously, we demonstrated that the responses of V4 neurons to naturalistic texture patches are sensitive to four key features of human texture perception: coarseness, directionality, regularity, and contrast. To begin to understand how distinct texture perception emerges from the dynamics of neuronal responses, in 2 macaque monkeys (1 male, 1 female), we investigated the relative contribution of the four texture attributes to V4 responses in terms of the strength and timing of response modulation. We found that the different feature dimensions are associated with different temporal dynamics. Specifically, the response modulation associated with directionality and regularity was significantly delayed relative to that associated with coarseness and contrast, suggesting that the latter are fundamentally simpler feature dimensions. The population of texture-selective neurons could be grouped into multiple clusters based on the combination of feature dimensions encoded, and those subpopulations displayed distinct temporal dynamics characterized by the weighted combinations of multiple features. Finally, we applied a population decoding approach to demonstrate that texture category information can be obtained from short temporal windows across time. These results demonstrate that the representation of different perceptually relevant texture features emerge over time in the responses of V4 neurons. The observed temporal organization provides a framework to interpret how the processing of surface features unfolds in early and midlevel cortical stages, and could ultimately inform the interpretation of perceptual texture dynamics.\nFigures #Fig1. Visual stimuli # We used a set of 21 textures to study responses of V4 neurons. The ordering of textures along the four dimensions is shown: (A) coarseness, (B) directionality, (C) regularity, and (D) contrast. Along each axis, textures are rank-ordered; in A from fine to coarse, etc. Numbers below each texture image indicate the raw index value along each axis. Gray triangles represent the corresponding z-scored value. Red triangles represent the median texture and the corresponding z-scored value along each axis. Each texture was presented in four orientations for a total of 84 stimuli (see Materials and Methods). Fig2. Example neuron selective for coarse textures # A, PSTHs are shown for the 84 texture stimuli. All panels represent the same data but differently ordered in accordance with the rank-ordering in Figure 1 (i.e., in ascending order of the index value along each of the four texture dimensions). From top to bottom for each panel, textures run from fine to coarse, nondirectional to directional, irregular to regular, and low-contrast to high-contrast, respectively. Color represents response strength in accordance with the scale bar. Responses are smoothed with a Gaussian of œÉ = 5 ms. B, Average PSTHs for the top and bottom halves along each texture dimension are shown in blue and red, respectively. For example, in the first panel at left, blue and red represent responses to fine and coarse textures, respectively. Black asterisks indicate time points with significant difference between red and blue curves (Mann‚ÄìWhitney U test in a 30 ms sliding window, p \u0026lt; 0.05). Statistically significant difference between responses to coarse and fine textures emerged 41 ms after stimulus onset. C, The 20 most (top) and least (bottom) preferred textures based on the number of spikes during the 50-150 ms window after stimulus onset (shading in B) are shown. D, Scatter plots represent neuronal responses to all texture stimuli during the 50-150 ms window as a function of each texture index. Filled symbols represent the 20 preferred and nonpreferred textures shown in C. Fig3. Example neuron selective for texture contrast # Low-contrast textures evoked stronger responses from Neuron 2 compared with high-contrast textures (A,B, rightmost panels). Statistically significant difference between responses to low- and high-contrast textures emerged 43 ms after stimulus onset. A mild preference for coarse textures is notable later in the response. All conventions are as in Figure 2. C, The 20 most and least preferred textures are shown based on the number of spikes during the 50-150 ms window after stimulus onset (shading in B). D, Scatter plots represent neuronal responses to all texture stimuli during the 50-150 ms window as a function of each texture index. Filled symbols represent the 20 preferred and nonpreferred textures shown in C. Fig4. Example neuron selective for nondirectional textures # A, B, Textures lacking directional information evoked stronger responses from Neuron 3. Statistically significant difference between responses to nondirectional and directional textures emerged 109 ms after stimulus onset. C, The 20 most and least preferred textures are shown based on the number of spikes during the 100-400 ms window after stimulus onset (shading in B). All conventions are as in Figure 2. D, Scatter plots represent neuronal responses to all texture stimuli during the 100-400 ms window as a function of each texture index. Filled symbols represent the 20 preferred and nonpreferred textures shown in C. Fig5. Example neurons selective for multiple texture features # A, B, Coarse and irregular textures evoke strong responses from Neuron 4. Statistically significant difference between responses to coarse and fine textures emerged 52 ms after stimulus onset, whereas that between regular and irregular textures emerged later, at 100 ms. Statistically significant difference in response between directional and nondirectional textures is also evident, but this can be explained by the correlation between directionality and regularity in our stimulus set (see Results). C-F, Scatter plots show mean neuronal responses to all texture stimuli during the 50-150 ms (D) or 100-400 ms (F) epoch after stimulus onset as a function of each texture index. The 20 most and least preferred textures based on early activity (C) and later activity (E) are shown. All conventions are as in Figure 2. Fig6. Population results: relative contribution of different texture attributes # A, Fitted weights for the four texture attributes based on the stepwise regression analysis for individual neurons are shown (see Materials and Methods). Models were based on neuronal responses in the 50-400 ms window after stimulus onset. Red and blue represent positive and negative weights, respectively (see color bar). Grayscale represents goodness of fit in terms of the correlation coefficient (Pearson\u0026rsquo;s r) between the observed and predicted responses for each neuron. Neurons (N1-N4) corresponding to examples in Figure 2-5 are identified. B, Pie plot represents relative proportions (and numbers) of neurons encoding single or multiple texture features based on the number of coefficients deemed significant in the stepwise regression model. C, Tabulation of the frequency of the different combinations of feature dimensions that provided the best fit for neuronal responses based on the regression models. D, Distribution of goodness-of-fit values quantified by the correlation coefficient (r) between the observed and predicted data values from the 108 neurons with a statistically significant stepwise regression fit. Red dashed line indicates the median value, 0.50. E, The relative proportions of positive and negative weights for each texture attribute. Fig7. Population results: temporal dynamics of the stepwise regression fit # A, The fitted weights for the four attributes based on the stepwise regression model fit for individual neurons. Models were based on neuronal responses in the 50-400 ms window after stimulus onset (same as in Fig. 6A). B, Weights as a function of time for the four texture attributes based on the stepwise regression fit. Models were based on neuronal responses within a 100 ms sliding window from ‚Äì100 to 500 ms sliding in 10 ms increments. Red and blue represent positive and negative weights, respectively. C, Variance in texture weights across neurons (Var_wt) is shown as a function of time for each of the four texture dimensions. High values of Varwt are an indicator of the emergence of selectivity. D, Var_wt, normalized by the maximum across texture attributes, is plotted to facilitate direct comparison of the temporal dynamics. Varwt for coarseness and contrast rose rapidly and reached the maximum values no later than 100 ms after stimulus onset, whereas those for directionality and regularity evolved more slowly and reached the maximum values at ‚àº150 ms after stimulus onset. Fig8. Temporal dynamics: the frequency of significantly modulated neurons # A-D, The number of significantly modulated neurons as a function of time for each texture dimension. Red lines at 100 ms after the stimulus onset are included to facilitate comparison across panels. Fig9. Decoding of texture category from population responses # A, Decoding accuracy of SVM classifiers for each of the four texture dimensions is plotted as a function of population size. At each size, neurons were sampled with replacement to generate a simulated subpopulation, and an SVM was trained to assign each texture to one of two categories along each texture dimension (e.g., coarse vs fine) based on spiking responses during the 50-400 ms window after stimulus onset. The simulation was repeated 100 times, and the average cross-validation scores were obtained for each texture dimension. Error bars indicate SEM. B, Time course of SVM classifier performance was quantified using a sliding window (bin width: 30 ms, step size: 1 ms) for a population size of 60 neurons. The simulation was repeated 100 times, and the average cross-validation scores were obtained for each texture dimension. Shaded area represents ¬± 1 SEM. Arrows indicate the peak proportion correct for each curve. ","date":"26 January 2022","permalink":"/publications/jneurosci_2022/","section":"Selected publications","summary":"Taekjun Kim, Wyeth Bair, Anitha Pasupathy \u003ccode\u003eJournal of Neuroscience\u003c/code\u003e","title":"Perceptual Texture Dimensions Modulate Neuronal Response Dynamics in Visual Cortical Area V4"},{"content":"","date":null,"permalink":"/tags/temporal-dynamics/","section":"Tags","summary":"","title":"Temporal Dynamics"},{"content":"","date":null,"permalink":"/tags/texture/","section":"Tags","summary":"","title":"Texture"},{"content":"","date":null,"permalink":"/tags/object-recognition/","section":"Tags","summary":"","title":"Object Recognition"},{"content":"","date":null,"permalink":"/tags/review/","section":"Tags","summary":"","title":"Review"},{"content":"Article info # Authors Anitha Pasupathy, Dina V Popovkina, Taekjun Kim Publication date 2020/09/15 Journal Annual Review of Vision Science DOI https://doi.org/10.1146/annurev-vision-030320-041306 Abstract #Area V4‚Äîthe focus of this review‚Äîis a mid-level processing stage along the ventral visual pathway of the macaque monkey. V4 is extensively interconnected with other visual cortical areas along the ventral and dorsal visual streams, with frontal cortical areas, and with several subcortical structures. Thus, it is well poised to play a broad and integrative role in visual perception and recognition‚Äîthe functional domain of the ventral pathway. Neurophysiological studies in monkeys engaged in passive fixation and behavioral tasks suggest that V4 responses are dictated by tuning in a high-dimensional stimulus space defined by form, texture, color, depth, and other attributes of visual stimuli. This high-dimensional tuning may underlie the development of object-based representations in the visual cortex that are critical for tracking, recognizing, and interacting with objects. Neurophysiological and lesion studies also suggest that V4 responses are important for guiding perceptual decisions and higher-order behavior.\nFigures #Fig1. Interconnections between V4 and other brain regions # Areas connected with V4 are highlighted on the lateral surface of the macaque monkey brain. (a) Visual cortical areas along the ventral stream, many along the dorsal visual stream and frontal areas, and (b) the medial temporal lobe and many subcortical structures are interconnected with V4. Figure created using data from Felleman \u0026amp; Van Essen (1991), Parker (2007), Ungerleider et al. (2008), Gattass et al. (2014), and Ninomiya et al. (2012a,b). Part of the figure was adapted with permission from Pasupathy et al. (2018). Abbreviations: AIT, anterior inferotemporal area; CIT, central inferotemporal area; DP, dorsal prelunate area; DR, dorsal raphe; FEF, frontal eye field; FST, fundus of the superior temporal sulcus area; IP, intraparietal areas; LC, locus coeruleus; LGN, lateral geniculate nucleus; MR, medial raphe; MST, medial superior temporal area; MT, middle temporal area; nbM, basal nucleus of Meynert; PIP, posterior intraparietal area; PIT, posterior inferotemporal area; PO, parieto-occipital area; R, thalamic reticular formation; SC, superior colliculus; V3a, visual complex V3 part A; V4t, V4 transition zone; vlPFC, ventrolateral prefrontal cortex; VT, ventral tegmentum. Fig2. Distribution of response onset latencies of V4 neurons # (a) Onset latencies of 276 V4 neurons from three monkeys. Latency measurements were based on the responses of single V4 neurons in animals engaged in a passive fixation task. As animals fixated a variety of shape stimuli were presented within the receptive field of the cell under study. Peristimulus time histograms (PSTHs) were constructed based on evoked responses of each neuron and onset latencies were calculated based on the half-height method, i.e., as the time point at which the PSTH exceeded the mean of the peak and baseline rates. Onset latencies ranged from 25 ms to 200 ms; mean latency was 76.6 ms. For further details, see Zamarashkina et al. (2020). (b) PSTHs of two example V4 neurons studied during passive fixation, displaying approximately 100 ms difference in onset latency. PSTHs were aligned on stimulus onset and smoothed with a Gaussian kernel (œÉ = 4 ms). Fig3. Position-invariant tuning for boundary conformation in V4 # (a) Examples of tuning for boundary conformation. Responses of two example V4 neurons (top and bottom) to simple 2D shapes recorded in a fixating animal (for further details, see Kim et al. 2019a). Response frequency histograms (right) show that the shape stimuli evoked a broad range of responses from both neurons. The 20 shapes that evoked the strongest (PREF, red) and weakest (NPREF, blue) responses are shown for each neuron (left). For the top neuron, all of the preferred shapes included a medium-sharp convex feature to the upper right; shapes that evoked a weak response did not include this feature (compare PREF and NPREF shapes). For the second neuron, preferred but not nonpreferred shapes included a concavity at the top of the shape. Responses of both neurons can be well-explained by a two-dimensional angular position √ó boundary curvature model that captures the conformation of the shape boundary (Kim et al. 2019a). Panel adapted from Kim et al. (2019a) (CC BY 4.0). (b) Example of position-invariant shape tuning. This neuron shows strong narrow tuning for the orientation of a shape stimulus, responding best when the sharp convex projection is to the lower left. Shape selectivity was consistent irrespective of the absolute position of the shape within the receptive field (compare tuning for the different lines). The response magnitude varied across position, but the preference remained the same. Fig4. Object-centered tuning for boundary conformation in V4 # (a) Preference for a sharp convexity pointing up could be achieved by pooling signals from units tuned to an appropriate combination of orientations (feature template) and repeating the template at multiple positions [translation in the receptive field (RF)]. (b) This model would predict similar responses to the two shapes shown, one with a convex projection at the top of the shape and the other with a concave contour at the bottom of the shape, and would therefore not represent an object-centered code. (c) Example of object-centered tuning for contour conformation. Responses of this neuron show clear rotation tuning for a shape with a concave feature along the boundary. Responses are strongest (orange arrow) when the concavity is to the right of shape center. When the same boundary conformation forms a convex projection to the left of shape center, responses are weak (blue arrow). Thus, responses are not dictated by contour conformation alone. Fig5. Representation of real but not accidental object boundaries in V4 # (a, b) Accidental versus real contours. The angle Œò in panel a is a real contour, while in panel b, it is formed by the accidental occlusion of one circle by another. Accidental contours bound the occluded object, but they carry no information about its true shape, and they are perceptually discounted. (c) Responses of two example neurons that encode real but not accidental contours. We studied responses of neurons to crescent shapes in eight orientations (x axis), either in isolation (orange) or in combination with a circle (magenta); the circle was also presented in isolation (blue) at the same eight positions in the receptive field as in the combination stimulus. The neuron on the left responds preferentially to shapes with a sharp convexity at the bottom, as reflected in the tuning curve for the crescent alone (orange). When the crescent is formed by partial occlusion in the case of the combination stimulus, responses are suppressed (magenta) because the preferred sharp convexity is now an accidental feature. The neuron on the right responds best to shapes with a broad convexity to the upper right, consistent with the strong responses to the circle alone (blue). In this case, responses are not suppressed with the addition of a circle because the preferred broad convexity remains a real contour (for further details, see Bushnell et al. 2011b). (d) A population of V4 neurons can provide a complete representation of isolated shapes based on the component contour features. For example, the cat may be encoded by V4 neurons selective for sharp convexities to the upper right and upper left; concavity to the top; and broad convexities to the right, left, and bottom (see Pasupathy \u0026amp; Connor 2002). Panels a‚Äìc adapted from Bushnell et al. (2011b) (CC BY-NC-SA 3.0). Fig6. Joint encoding of multiple features for object segmentation # The input image on the left includes a variety of cues; the bottom images are filtered to include information from each cue in isolation and illustrate that form information may be encoded by a contrast in luminance, color, or texture. When viewing such a stimulus, responses of individual V4 neurons are dictated by tuning in a high-dimensional stimulus space defined by shape, luminance, color, texture, blur, depth, etc., which facilitates the effective segmentation of visual objects from the background that may be defined by contrast along a variety of stimulus dimensions. Fig7. Goal-oriented representations # (a) When confronted with the challenge of spotting bananas in a cluttered produce aisle, the subject may saccade to different locations with yellow objects (dashed trajectory) and compare the shape of the object at the attentional focus (circles) with a remembered object. Area V4 is thought to be important for all aspects of this process. (b) Size illusion. The retinal sizes of the two sasquatches in this image are identical, but the perceived sizes are dramatically different. This is because the surrounding context suggests that the sasquatch at right is farther away from the observer; thus, the same retinal size would imply a much larger sasquatch farther away. Fig8. Interactions between the visual and frontal cortex during shape discrimination # (a, b) Responses of two example V4 neurons that exhibit different response profiles to partially occluded shape stimuli. PSTHs in panel a exhibit two transient peaks (black and red bars), while those in panel b exhibit only one transient peak (black bar). During the first transient in both neurons, responses decline with increasing levels of occlusion (line colors; decreasing percent visible area). During the second transient in panel a, which may be based on feedback from frontal cortex, responses are stronger for intermediate levels of occlusion (for further details, see Fyall et al. 2017, Kosai et al. 2014). Adapted from Fyall et al. (2017) (CC-BY). (c, d) Psychometric and neurometric curves based on the data in panels a and b, respectively. In both, psychometric performance (dark gray dotted line) declines with increasing levels of occlusion. The neurometric curve based on a larger time window (orange line) shows improved performance at intermediate levels of occlusion for the neuron in panel a but not the one in panel b due to the enhanced shape selectivity for occluded stimuli during the second transient peak. Fig9. Memory encoding in V4 # Responses of an example neuron during the performance of a sequential shape-discrimination task are shown. Stimulus 1 was presented at central fixation (outside the receptive field of the neuron), followed by an inter-stimulus interval (ISI), and stimulus 2 was presented within the receptive field (RF). The animal had to report whether stimuli 1 and 2 were the same or different. Stimulus 1 and stimulus 2 could be shapes A or B, for a total of four conditions (for details, see Kosai et al. 2014). Responses of this neuron include three task-relevant pieces of information. First, the responses provide a sensory representation of stimulus 2 with stronger responses for shape A than shape B (compare the blue solid line and orange dashed line with the other two lines). Second, a memory representation of stimulus 1 is also evident during the ISI (blue arrow), with stronger responses when stimulus 1 was shape B (blue lines) rather than shape A (orange lines). Finally, responses also reflect whether stimuli 1 and 2 were the same or different with stronger responses during the presentation of stimulus 2 when stimuli 1 and 2 were different (solid lines, black arrows). ","date":"15 September 2020","permalink":"/publications/annurevvissci_2020/","section":"Selected publications","summary":"Anitha Pasupathy, Dina V Popovkina, Taekjun Kim \u003ccode\u003eAnnual Review of Vision Science\u003c/code\u003e","title":"Visual functions of primate area V4"},{"content":"","date":null,"permalink":"/tags/boundary/","section":"Tags","summary":"","title":"Boundary"},{"content":"Article info # Authors Anitha Pasupathy, Taekjun Kim, Dina V Popovkina Publication date 2019/10/01 Journal Current opinion in neurobiology DOI https://doi.org/10.1016/j.conb.2019.09.009 Abstract #Recognizing a myriad visual objects rapidly is a hallmark of the primate visual system. Traditional theories of object recognition have focused on how crucial form features, for example, the orientation of edges, may be extracted in early visual cortex and utilized to recognize objects. An alternative view argues that much of early and mid-level visual processing focuses on encoding surface characteristics, for example, texture. Neurophysiological evidence from primate area V4 supports a third alternative ‚Äî the joint, but independent, encoding of form and texture ‚Äî that would be advantageous for segmenting objects from the background in natural scenes and for object recognition that is independent of surface texture. Future studies that leverage deep convolutional network models, especially focusing on network failures to match biology and behavior, can advance our insights into how such a joint representation of form and surface properties might emerge in visual cortex.\nFigures #Fig1. Candidate models for object recognition # (a) Deficiencies of a shape-based approach. Example natural scenes (left), and the component edges in the scenes (right) as computed by a Canny edge detector. These examples show that object shape segmentation based on edge-detection becomes very difficult when region contrast information (color, luminance, or texture) is not available. (b) Deficiencies of a texture-based approach. Each row shows multiple synthesized images using the Portilla‚ÄìSimoncelli algorithm. They have the same higher-order texture statistics as the original images in (a), but global form is destroyed. (c) Framework for a new joint encoding model. The input image is first processed to extract V1-like features. Here the image is passed through a bank of 4 orientation √ó 2 spatial frequency filters, and the output is shown. Component objects in the image are extracted by identifying uniform image regions with K-means clustering (set to K = 2). Detailed processing of surface texture also starts from V1 outputs and runs in parallel. As proposed by Okazawa et al., the texture information is encoded by computing correlations in activity among neighboring neurons. This computation may be mediated by slow lateral cortical connections rather than fast feedforward connections. Fig2. Responses to filled and outline stimuli in V4 # (a),(b) Results from two example V4 neurons (a) and (b) and from an instantiation of the Hmax model (c) are shown. Each point represents the mean response of a single neuron to a filled (X-axis) and an outline (Y-axis) stimulus with the same boundary shape (inset: stimulus examples). We recorded the responses of each neuron to 362 shapes. (a) An example neuron that evoked a larger range, and more shape-selective responses to filled shapes than outline shapes. (b) An example neuron that responded strongly and selectively to outlines, but not to filled stimuli. (c) A typical HMax model unit responds strongly and selectively to both filled and outline stimuli. Importantly, the responses are highly correlated. (d),(e) Responsiveness to filled and outline stimuli in a population of 43 V4 neurons. (d) Standard deviation (SD) of normalized responses to outline stimuli plotted against SD of normalized responses to filled stimuli; each point represents one neuron. Most points lie below the diagonal indicating that dispersion of responses across shapes was greater for filled shapes. (e) Distribution of relative response range, œÅ, in the same population of neurons. For each neuron, this metric compares the responsiveness to outline versus filled stimuli (see Ref. [29‚Ä¢‚Ä¢] for further details). Dashed line: similar responsiveness to filled and outline stimuli; left of dashed line, more responsive to filled stimuli; right of dashed line, more responsive to outline stimuli. Triangle indicates median. Fig3. Responses to shapes, textures, and boundary blur in V4 # (a)‚Äì(c) Responses of example neurons to shapes (shown in each panel) with different textures painted on the surface. Line colors denote shape; textures are ordered along the X-axis in accordance with responses to the shape shown in red. For each neuron we chose a preferred shape (red) based on preliminary characterization, a less-preferred shape (gray) and a circle. Error bars indicate standard error of mean. (a) Example neuron with strong selectivity for shape but not texture. (b) Example neuron with strong selectivity for texture but not shape. All three line colors follow a similar trend. (c) Example neuron that is tuned to both shape and texture. In all three cases, tuning for shape and texture are separable and responses can be modeled as a product of tuning for shape √ó texture. (d),(e) Encoding of boundary blur in V4. (d) Stimuli with boundary blur. Blurring was achieved by applying a circular 2D Gaussian blur kernel to the shape. The numbers denote blur factor, which represents the standard deviation of the Gaussian kernel in units relative to the radius of the circle shape (see panel c). (e),(f). Two example neurons that exhibit tuning for blur. Responses to preferred (more red) and non-preferred (more blue) shapes are plotted as a function of the blur factor. Both neurons are selective for intermediate levels of blur. Tuning for blur was independent of the shapes used for this and other neurons [35‚Ä¢‚Ä¢]. Error bars indicate standard error of mean. This figure is adapted with permission from [35‚Ä¢‚Ä¢] under the Creative Commons Attribution 4.0 International License. Fig4. Deep convolutional neural nets and the primate ventral visual pathway # (a) Schematic of a macaque brain (side view) indicating ventral visual areas V1, V2, V4, and inferotemporal cortex (IT; PIT: posterior IT; CIT: central IT; AIT: anterior IT). (b) Schematic of AlexNet, an example deep convolutional network. Color coding identifies stages of the network and primate brain areas in (a) hypothesized to be analogous based on published results [V1: [65]; V4: [66‚Ä¢]; IT: [63]). (c),(d). Examples of images leading to classification failure in deep nets. (c). A conflict between texture and shape in the input image, generated by style transfer between the first two images [68‚Ä¢‚Ä¢,69], produces texture-based classification of a camel as an elephant. (d) Adversarial noise [70] produces misclassification of a modified input image that is indistinguishable from the original by human observers. In this case, a macaque monkey is misclassified as a pineapple with considerable certainty despite minimal difference between the original and modified image. ","date":"1 October 2019","permalink":"/publications/curropinneurobiol_2019/","section":"Selected publications","summary":"Anitha Pasupathy, Taekjun Kim, Dina V Popovkina \u003ccode\u003eCurrent opinion in neurobiology\u003c/code\u003e","title":"Object shape and surface properties are jointly encoded in mid-level ventral visual cortex"},{"content":"","date":null,"permalink":"/tags/shape/","section":"Tags","summary":"","title":"Shape"},{"content":"","date":null,"permalink":"/tags/surface/","section":"Tags","summary":"","title":"Surface"},{"content":"Article info # Authors Taekjun Kim, Wyeth Bair, Anitha Pasupathy Publication date 2019/06/12 Journal Journal of Neuroscience DOI https://doi.org/10.1523/JNEUROSCI.3073-18.2019 Abstract #The distinct visual sensations of shape and texture have been studied separately in cortex; therefore, it remains unknown whether separate neuronal populations encode each of these properties or one population carries a joint encoding. We directly compared shape and texture selectivity of individual V4 neurons in awake macaques (1 male, 1 female) and found that V4 neurons lie along a continuum from strong tuning for boundary curvature of shapes to strong tuning for perceptual dimensions of texture. Among neurons tuned to both attributes, tuning for shape and texture were largely separable, with the latter delayed by ‚àº30 ms. We also found that shape stimuli typically evoked stronger, more selective responses than did texture patches, regardless of whether the latter were contained within or extended beyond the receptive field. These results suggest that there are separate specializations in mid-level cortical processing for visual attributes of shape and texture.\nFigures #Fig1. Visual stimuli # Visual stimuli. A, Shape stimuli. We used a subset (30 of 51) of the 2D shapes developed by Pasupathy and Connor (2001) to study how boundary conformation influences V4 responses. Most shapes were presented at 8 rotations (45¬∞ increments); a few shapes (those identified with a superscript) were presented at fewer rotations (1, 2, or 4, as noted in figure) due to rotational symmetry. The circle was presented at three luminance contrast levels (1, 16, 46 cd/m2) relative to the background (8 cd/m2), for a total of 225 shape stimuli. B, Texture stimuli. We constructed eight (23) texture categories based on three dimensions that influence human texture perception (coarse vs fine, directional vs nondirectional, regular vs irregular), and selected 2‚Äì3 representative textures for each category (see Materials and Methods). Each texture was presented through a circular aperture of two sizes and at four orientations at 45¬∞ increments, for a total of 168 texture stimuli. C, Examples of shape, large aperture texture, and small aperture texture stimuli. All parts of the shape stimulus were within the estimated RF region (yellow dotted line). The diameter of the large aperture texture stimuli was twice that of the estimated RF. Small aperture texture condition was created by applying a RF sized circular aperture to large aperture texture. Fig2. Example neurons and population results # A, Response frequency histogram for shape (top row) and texture (bottom row) stimuli for 6 example neurons (columns). Red and blue histograms represent responses to small and large aperture textures, respectively. Responses for each neuron were normalized to the maximum across all shape and texture stimuli (maximum values are shown for each neuron). Triangles represent the background responses (no visual stimulus). B, Maximum response across all shape stimuli (x axis) is plotted against the maximum response across all small aperture texture stimuli (y axis) for each neuron from Monkey 1 (black) and Monkey 2 (gray). Filled symbols represent neurons with a statistically significant difference between the strongest shape and texture response, assessed with a permutation test (see Materials and Methods). In both monkeys, the maximum response across shape stimuli was typically greater than the maximum response across texture stimuli. C, SD of the response frequency histogram for shape (x axis) is plotted against that for texture stimuli (y axis). Asterisks indicate mean value. Yellow highlight represents region where SD ratio for shape versus texture lies between 2/3 and 3/2. Points corresponding to examples in A are identified. D, Histogram of the SD ratio: SDshape/SDtexture. Yellow highlight as in C. E, F, SD values for shuffled shape and texture responses. Shape and texture responses for each neuron were shuffled and SDs were recomputed for two randomly divided groups: Group 1 (N = 225, number of shapes) and Group 2 (N = 84, number of textures).F, Gray bars represent SD ratios computed from E. This process was repeated 10,000 times and width of the distribution from the observed data, quantified by the interquartile range of log (SD ratio), was always at least 5 times broader than that from the shuffled data. Fig3. HMax model prediction of responses to texture stimuli # A, The top 20 preferred shape and texture stimuli of an example neuron (#7). B, Shape template for the S2 unit corresponding to the best fitting HMax model based on the responses to shape stimuli. Each ellipse indicates position, orientation, and size of complex-cell like subunit (C1 unit). Grayscale represents weighting strength with darker color denoting stronger weight. C, Predicted responses (y axis) based on the best HMax model fit (shown in B) for shape (gray) and texture (red) stimuli are plotted against measured responses (x axis). For this neuron, the HMax model provided an excellent fit for shape responses, but not for texture responses. Predicted texture responses showed a much broader range than the observed data. D‚ÄìF, The results from another example neuron. The same conventions as in A‚ÄìC. Fig4. SD ratios from HMax model prediction # A‚ÄìD, Population results for HMax models optimized based on shape responses only. Model goodness of fit for shape (A) and texture (B) responses across all neurons. Goodness of fit was determined as the median correlation coefficient (r) of 10-fold cross-validation test sets. Triangles represent median. HMax models provided a good fit for shape responses (median r = 0.61) but a poor fit for the texture responses (median r = 0.02). Predicted response ranges (SD values) for shape and texture stimuli were similar (C), and the SD ratios (gray bars in D) spanned a narrow range. SD ratios from the observed data (as in Fig. 2D) are overlaid in white (D) for comparison. D, Light gray bars represent overlap between gray and white distributions. White bars are all the same in D, H, L. Yellow shaded area as in Figure 2. E‚ÄìH, HMax model results optimized based on texture responses only. HMax models provided a poor fit for both shape (median r is 0.07) and texture (median r is 0.23) responses. Model results in terms of SD values and ratios (G, H) were similar to those in C, D. I‚ÄìL, HMax models optimized simultaneously based on shape and texture responses. Models provided a good fit for shape responses (median r = 0.56) but a poor fit for texture responses (median r = 0.10). In this case, the distribution of SD ratios were similar to V4 data (compare gray and white bars in L), but the SDs for predicted texture responses were unlike observed data: note the lack of low (\u0026lt;0.05) and high (\u0026gt;0.2) SDs for texture in K compared with Figure 2C. Asterisk indicate mean value. Fig5. Tuning for boundary curvature in shape-selective neurons # A, Shape stimuli that evoked the strongest (preferred) and weakest (nonpreferred) responses from an example neuron (#2; also in Fig. 2). For this neuron, shapes evoked a broader range of responses than textures: SD for shape = 0.23; SD for texture = 0.07. B, Responses to shape stimuli were best explained by a 2D Gaussian APC model with a peak at a curvature of ‚àí0.27 at 90¬∞, reflecting the preference for concave curvature to the top of the shape. C, Responses predicted by the best fit APC model (y axis) are well correlated with the observed responses. D‚ÄìF, Results from a second example neuron (#8). The same conventions as in A‚ÄìC. This neuron responded strongly to shapes with a concave contour at top right of the shape (45¬∞). SD for shape = 0.22; SD for texture = 0.02. G, Neurons whose responses are well predicted by the APC model (filled symbols, APC model goodness of fit \u0026gt; 0.6) are identified on a scatter plot of response range for shape and texture stimuli (same as Fig. 2C). This included 42 of 101 neurons across our dataset (right, histogram). Top right, Histogram represents the distribution of SD differences (shape ‚àí texture) for highly shape-selective (black bars) and other neurons (white). Mean SD shape minus SD texture was significantly different for the two groups of neurons (Mann‚ÄìWhitney U test, p \u0026lt; 0.001). Triangles represent median values of the distributions. Gray bars represent overlap between two distributions. Data points corresponding to the example neurons in A‚ÄìC (#2), and D‚ÄìF (#8) are identified. Asterisks indicate that difference between two distributions are statistically significant at the level of p \u0026lt; 0.001 (Mann-Whitney U-test). Fig6. Tuning for perceptual dimension of texture # A, Texture stimuli that evoked the strongest (preferred) and weakest (nonpreferred) responses from an example neuron (#9). The nonpreferred textures are directional, oriented at different directions, unlike the preferred stimuli, which tend to be nondirectional for this neuron. SD for shape = 0.13; SD for texture = 0.20. B, Neural responses for all texture stimuli (y axis) plotted as a function of the directionality index (x axis) shows a statistically significant (p \u0026lt; 0.001) negative correlation. C, D, Example neuron (#10) that responded strongly to coarse rather than fine textures. SD for shape = 0.08; SD for texture = 0.11. E, Neurons whose responses are well predicted by the texture model (see Materials and Methods; filled symbols, texture model goodness of fit \u0026gt; 0.6) are identified on the scatter plot of response range for shape and texture stimuli. This included 27 of 101 neurons across our dataset (right, histogram). These texture-selective neurons (filled circles) and the other neurons (open circles) showed a significant difference in distribution of shape SD minus texture SD (Mann‚ÄìWhitney U test, p \u0026lt; 0.001; see top right, histogram). Triangles represent median values of the distributions. There was limited overlap (n = 6) between neurons with APC model goodness of fit \u0026gt; 0.6 and those with texture model goodness of fit \u0026gt; 0.6 (compare filled symbols in Fig. 5G vs Fig. 6E). Data points corresponding to the example neurons in A and B (#9), and C and D (#10) are identified. Fig7. Joint coding of shape and texture # A, Responses of an example neuron (#11) to 10 nondirectional textures (x axis) presented through three different shapes apertures (line colors). Responses to the three shapes presented with a uniform luminance contrast are also shown (leftmost symbols) for comparison. Error bars indicate ¬±1 SEM. This neuron exhibited a broader range of shape responses than texture responses (SD for shape = 0.19, SD for texture = 0.10), but overall, shape preference was largely preserved across textures. B, Example neuron (#12) with a strong preference for texture but not shape (SD for shape = 0.07, SD for texture = 0.16). All details as in A. C, Neuron 8 showed selectivity along shape and texture dimensions. Preference for fine textures was observable only within the preferred shape boundary. D‚ÄìF. Additional example neurons (#13, #14, #15) that exhibited joint tuning for shape and texture. G, Effect size (see Materials and Methods) of texture was compared with that of shape for each of 43 neurons subjected to the control experiment. Data points corresponding to the example neurons (A‚ÄìF) are identified. H, To quantify the independence of shape and texture tuning, we evaluated whether responses to shape-texture combination stimuli can be predicted by the product of the responses to shape and texture. Scatter plots show observed responses versus those predicted by a multiplicative model (see Materials and Methods) for neurons in A‚ÄìF. I, Comparison between multiplicative (x axis) model and additive (y axis) model. Goodness-of-fit (r) values were quantified by the correlation coefficient between observed and predicted responses across all neurons (n = 43). Multiplicative model (median r = 0.91) generally provides a better fit than an additive model (median r = 0.86). Asterisk indicate median value. Fig8. Temporal dynamics of shape and texture selectivity # A, PSTHs for shape (left), large and small aperture textures (middle and right, respectively) are shown for preferred (red; top 50% of stimuli based on spike counts between 50 and 400 ms after stimulus onset; see Materials and Methods), nonpreferred (blue; bottom 50%), and all stimuli (black) for one example neuron. Shaded area represents ¬±1 SEM. For shape stimuli, difference in responses between preferred and nonpreferred stimuli emerged soon (50 ms) after response onset. For textures, statistically significant difference emerged at 100 ms after stimulus onset. B, Second example neuron showing delayed emergence of texture selectivity (shape-dependent modulation ‚â• 51 ms; texture-dependent modulation ‚â• 88 ms; earlier onset was determined for small aperture texture condition). C, Across all neurons, onset of shape selectivity (x axis) is plotted against onset of texture selectivity (y axis). Filled and open symbols represent large and small aperture conditions, respectively. Data points from the same neuron are connected by a vertical line. In a few neurons (data points without vertical line), onset of texture selectivity could not be defined for one of the aperture conditions due to weak responses. Most data points lie above the diagonal line, indicating that texture information is processed later than shape information. D, Marginal histograms for onset times for shape (gray), large aperture texture (black), and small aperture texture (white). Triangles represent the mean of each distribution (shape = 55.72 ms, large aperture texture = 85.53 ms, small aperture texture = 85.78 ms). On average, onset of shape selectivity was ‚àº30 ms faster than onset of texture selectivity. ","date":"12 June 2019","permalink":"/publications/jneurosci_2019/","section":"Selected publications","summary":"Taekjun Kim, Wyeth Bair, Anitha Pasupathy \u003ccode\u003eJournal of Neuroscience\u003c/code\u003e","title":"Neural coding for shape and texture in macaque area V4"},{"content":"","date":null,"permalink":"/tags/binocular/","section":"Tags","summary":"","title":"Binocular"},{"content":"Article info # Authors Taekjun Kim, Ralph D Freeman Publication date 2017/02/01 Journal European Journal of Neuroscience DOI https://doi.org/10.1111/ejn.13500 Abstract #The fine task of stereoscopic depth discrimination in human subjects requires a functional binocular system. Behavioral investigations show that relatively small binocular abnormalities can diminish stereoscopic acuity. Clinical evaluations are consistent with this observation. Neurons in visual cortex represent the first stage of processing of the binocular system. Cells at this level are generally acutely sensitive to differences in relative depth. However, an apparent paradox in previous work demonstrates that tuning for binocular disparities remains relatively constant even when large contrast differences are imposed between left and right eye stimuli. This implies a range of neural binocular function that is at odds with behavioral findings. To explore this inconsistency, we have conducted psychophysical tests by which human subjects view vertical sinusoidal gratings drifting in opposite directions to left and right eyes. If the opposite drifting gratings are integrated in visual cortex, as wave theory and neurophysiological data predict, the subjects should perceive a fused stationary grating that is counter-phasing in place. However, this behavioral combination may not occur if there are differences in contrast and therefore signal strength between left and right eye stimuli. As expected for the control condition, our results show fused counter-phase perception for equal inter-ocular grating contrasts. Our experimental tests show a striking retention of counter-phase perception even for relatively large differences in inter-ocular contrast. This finding demonstrates that binocular integration, although relatively coarse, can occur during substantial differences in left and right eye signal strength.\nFigures #Fig1. Previous studies: effects of unequal monocular contrast on disparity sensitivity (psychophysics) and binocular phase tuning (neurophysiology). # (A) Replotted from Legge \u0026amp; Gu (1989). Disparity threshold was measured as a function of spatial frequency and unequal monocular contrast. Human subjects were provided with four panels of vertical sign-wave gratings. Left and right eyes could see only the left and right columns of the panels respectively. The bottom pair of grating panels formed a reference stereo image (zero disparity). The top pair of grating panels, which are identical to the reference except for spatial phase, formed a near (crossed disparity, phase-shifted inward as shown at the top of A) or far (uncrossed disparity) target stereo image. When an equal stimulus contrast (25%) was used for left and right eye stimulation, subjects could easily detect a target image with a small disparity. However, as stimulus contrast for one eye became higher (i.e., unequal monocular contrast), the disparity detectability of a target was gradually impaired. Similar effects were found for two spatial frequencies as shown. (B) Neurophysiological data from single cells in visual cortex. Gratings at optimal orientation and spatial frequency are presented at 50% contrast to left and right eyes as relative intraocular phase is varied. (B top). A similar test is shown (B bottom) for the same cell in which contrast of the grating for one eye is reduced by a log unit to 5%. Although overall response is reduced, relative phase tuning is closely similar to that for equal left and right eye contrast stimulation. Depth of modulation (DM), computed as illustrated in the upper right, is plotted against varied- eye contrast (lower right). As shown, DM is nearly flat across a range of contrast differences between left and right eyes. For comparison, a contrast response function is shown for the left eye alone. Fig2. Apparatus and stimuli # (A) A mirror haploscope is used to present two separate views to each eye. It consists of two pairs of vertically mounted mirrors (gray rectangles) and a pair of screens (black rectangles) in front of them toward the visual stimulation monitor. The angles of the two large mirrors and horizontal positions of the two screens are adjusted by individual subjects to achieve optimal fusion of left and right eye images. In the depicted situation, the left eye is stimulated with a vertical sign-wave grating drifting rightward. The grating stimulus in the right visual field is the same as that for the left, but it moves in the opposite (leftward) direction. Binocular fusion of these two stimuli is expected to result in the perception of a counter-phase flickering grating. (B) Each trial begins with a view of binocular-fusion-assisting frames and a dichoptic cross (Ding \u0026amp; Levi, 2011). Left and right frames contain opposite halves of the cross, which are combined during optimal fusion. Subjects are instructed to try to maintain perception of a red cross within a green circle just before activating the space bar to report detection of the counter-phase grating. A beep signals presentation of two vertical sign-wave gratings, which are identical except for contrast. (One is fixed at 48%, and the other has the same or a lower contrast. The eye presented with a higher contrast is varied). At first, the gratings move in the same direction at 0.5 cycles/s. Position of the higher contrast grating and direction of motion (left or right) are randomly chosen in each trial. After a delay of 2‚Äì3 s, also randomly chosen, a direction change occurs for one of the two grating stimuli. For each trial, H‚ÄìL, H‚ÄìL, or H \u0026amp; L indicate higher or lower contrast stimuli, respectively. The bold character means that a direction change occurs for that stimulus so as to form a counter-phase grating. If the subject detects this change, they report it and abort the trial by pressing the space bar and the reaction time is recorded. On half of the total trials, direction change occurs for both (H‚ÄìL) and (H‚ÄìL) conditions of grating stimuli. For these conditions, subjects are instructed to wait until the trial ends (5 s after stimulus onset) without depressing the space bar. Inter-trial interval is 5 s. The number of total stimulus conditions is 80 (4 √ó 5 √ó 4): Spatial frequencies (0.25, 0.5, 1, 2 cycles/deg), contrasts (3, 6, 12, 24, 48%), direction changes (H‚ÄìL, H‚ÄìL, H‚ÄìL, H‚ÄìL). Fig3. Effects of unequal monocular contrast on detection of counter-phase gratings # (A, C, E, G) Color matrices in left column show performance levels of three individual subjects and their average or composite values for the counter-phase grating detection task as a function of stimulus contrast (x-axis) and spatial frequency (y-axis). The brighter the color, the higher the percentage of correct answers. (B, D, F, H) Each row of color matrix data in the left column is replotted to show effects of unequal monocular contrast on detectability of counter-phase gratings. Only two out of four sets of data are shown here for visual clarity (Black: 0.5 cycles/deg; Gray: 2 cycles/deg). Red points represent mean values for all spatial frequency conditions. Psychometric curves are fitted with Weibull functions, urn:x-wiley:0953816X:media:ejn13500:ejn13500-math-0001. For all subjects, percentages of correct answers are lowest for highest spatial frequency condition. However, the relationship between detectability of counter-phase gratings and spatial frequency is non-linear (see more detail in Fig. 4). Fig4. Effects of spatial frequency on detection of counter-phase gratings # (A, C, E, G) Color matrices in left column contain the same information as that in Fig. 3. But they are transformed to show stimulus contrast on the y-axis and spatial frequency on the x-axis. The brighter the color, the higher the percentage of correct answers. (B, D, F, H) Each row of the color matrix in the left column is replotted to show effects of spatial frequency on detectability of counter-phase gratings. The brighter the data point, the higher the contrast condition. Red points represent mean values for all contrast conditions. Note that the best performance is observed at 0.5 cycles/deg., which is intermediate in the spatial frequency range we used. Fig5. Reaction times for detection of counter-phase gratings # (A, C, E, G) Reaction time distributions for the counter-phase grating detection task, are plotted against grating stimulus contrast. For each boxplot, the thick horizontal line within the box is the median (50th percentile), and the top and bottom edges of the box specify the 25th and 75th percentiles of the data set, respectively. The sum of the vertical lines above and below the boxes covers 99% of the entire data distribution. The gray cross-marks outside this range indicate outliers. Reaction times are gradually reduced as stimulus contrast of the varied-eye increases. (B, D, F, H) Reaction time distributions for the counter-phase grating detection task are plotted against spatial frequency. The same conventions are used as in the left column. Reaction times tend to increase with higher spatial frequency values. ","date":"1 February 2017","permalink":"/publications/ejn_2017/","section":"Selected publications","summary":"Taekjun Kim, Ralph D Freeman \u003ccode\u003eEuropean Journal of Neuroscience\u003c/code\u003e","title":"Binocular function during unequal monocular input"},{"content":"","date":null,"permalink":"/tags/contrast/","section":"Tags","summary":"","title":"Contrast"},{"content":"","date":null,"permalink":"/tags/depth/","section":"Tags","summary":"","title":"Depth"},{"content":"","date":null,"permalink":"/tags/monocular/","section":"Tags","summary":"","title":"Monocular"},{"content":"","date":null,"permalink":"/tags/psychophysics/","section":"Tags","summary":"","title":"Psychophysics"},{"content":"","date":null,"permalink":"/tags/v1/","section":"Tags","summary":"","title":"V1"},{"content":"","date":null,"permalink":"/tags/cat/","section":"Tags","summary":"","title":"Cat"},{"content":"","date":null,"permalink":"/tags/direction-selectivity/","section":"Tags","summary":"","title":"Direction Selectivity"},{"content":"Article info # Authors Taekjun Kim, Ralph D Freeman Publication date 2016/05/01 Journal European Journal of Neuroscience DOI https://doi.org/10.1111/ejn.13223 Abstract #Neurons in the visual cortex are generally selective to direction of movement of a stimulus. Although models of this direction selectivity (DS) assume linearity, experimental data show stronger degrees of DS than those predicted by linear models. Our current study was intended to determine the degree of non-linearity of the DS mechanism for cells within different laminae of the cat\u0026rsquo;s primary visual cortex. To do this, we analysed cells in our database by using neurophysiological and histological approaches to quantify non-linear components of DS in four principal cortical laminae (layers 2/3, 4, 5, and 6). We used a DS index (DSI) to quantify degrees of DS in our sample. Our results showed laminar differences. In layer 4, the main thalamic input region, most neurons were of the simple type and showed high DSI values. For complex cells in layer 4, there was a broad distribution of DSI values. Similar features were observed in layer 2/3, but complex cells were dominant. In deeper layers (5 and 6), DSI value distributions were characterized by clear peaks at high values. Independently of specific lamina, high DSI values were accompanied by narrow orientation tuning widths. Differences in orientation tuning for non-preferred vs. preferred directions were smallest in layer 4 and largest in layer 6. These results are consistent with a non-linear process of intra-cortical inhibition that enhances DS by selective suppression of neuronal firing for non-preferred directions of stimulus motion in a lamina-dependent manner. Other potential mechanisms are also considered.\nFigures #Fig1. Comparisons of F1/F0 distributions and spontaneous activity among four laminar groups # A total of 899 cells in our database were assigned to one of four layer categories on the basis of histological reconstruction. We classified simple and complex cells on the basis of the F1/F0 ratio, and quantified spontaneous activity from each neuron. Vertical dotted lines (A‚ÄìD) indicate mean values of F1/F0 ratio distributions for four different layer groups. (A) F1/F0 distribution in layer 2/3. Fifty-five of 191 cells were classified as simple (F1/F0 ratio of \u0026gt;1). (B) F1/F0 distribution in layer 4. Two hundred and twenty of 281 cells were classified as simple. (C) F1/F0 distribution in layer 5. Seventeen of 128 cells were classified as simple. (D) F1/F0 distribution in layer 6. One hundred and 46 of 299 cells were classified as simple. (E) Spontaneous activity compared across layers 2/3, 4, 5, and 6. In each boxplot, the thick horizontal line within the box is the median (50th percentile), and the top and bottom edges of the box are the 25th and 75th percentiles of the dataset, respectively. The upper and lower vertical lines extend to cover 95% of the entire data distribution. The grey cross marks outside the range indicate outliers. Extreme outliers (\u0026gt;15 spikes/s) are omitted. Fig2. DSI distributions for four laminar groups # DSI values were calculated according to 1‚àí (np/p), where p and np indicate neural responses for preferred and non-preferred (180¬∞ away from the optimal value) directions, respectively. (A and B) DSI distribution for layer 2/3. Filled and unfilled bars represent simple cells (F1/F0 ratios of \u0026gt;1 or \u0026lt;1, respectively). The horizontal dotted line designates a mean uniform DSI distribution. (C and D) DSI distribution for layer 4. The same format is used as in A and B. (E and F) DSI distribution for layer 5. (G and H) DSI distribution for layer 6. Fig3. Linear predictions of DSI values made for three representative simple cells # (A) Peri-stimulus time histograms for preferred (black) and non-preferred (red) directions of motion were calculated for a simple cell in the granular layer. The numbers of spikes were counted for a 50-ms sliding window with 1-ms steps. The DSI value measured with moving grating stimuli for this cell was 0.82. (D) X‚ÄìT profile of the spatiotemporal linear RF of the simple cell described in A. Red colour with solid contour lines represents the bright excitatory (or ON) sub-region of the RF. Blue colour with dashed contour lines represents the dark excitatory (or OFF) sub-region of the RF. (G) A two-dimensional Fourier transform applied to the X‚ÄìT plot in D. The amplitude spectrum for positive temporal frequency reflects the neural response for rightward direction of motion. The contour map shows the best fit of Eqn 2. The DSI value predicted from this amplitude spectrum is 0.78. (B, E, and H) Linear prediction of the DSI value made for a simple cell in the supra-granular layer. The same conventions are used as in A, D, and G. (C, F, and I) Linear prediction of the DSI value is made for a simple cell in the infra-granular layer. The same conventions are used as in A, D, and G. Fig4. Comparisons of measured and predicted DSI values # (A) Simple (N = 61, filled circles) and complex (N = 15, open triangles) cells in layer 4. The values plotted on the x-axis and y-axis indicate DSI values measured with grating stimuli and those predicted from the spatiotemporal amplitude spectrum, respectively. The histogram in the upper right shows the distribution of prediction errors (measured DSI ‚Äì predicted DSI). The dashed line indicates the mean value of the distribution. Open and closed arrows indicate mean values of corresponding shaded sub-distributions. (B) Simple (N = 29) and complex (N = 20) cells in layer 2/3. The same format is used as in A. (C) Simple (N = 7) and complex (N = 14) cells in layer 5. The same format is used as in A. (D) Simple (N = 61) and complex (N = 31) cells in layer 6. The same format is used as in A. Note that the distributions of prediction errors are centred at zero for layer 4, but are clearly shifted to positive values for both supra-granular and infra-granular layers. Fig5. Comparisons of orientation tuning widths between preferred and non-preferred directions of motion # (A) Data from 66 units in layer 2/3. The x-axis and y-axis indicate orientation tuning widths (parameter œÉ of the Gaussian fitting curve) obtained for preferred and non-preferred directions of motion, respectively. Circles and triangles represent simple and complex cells, respectively. Direction-selective cells (filled symbols, DSI ‚â• 0.5) and non-direction-selective cells (open symbols, DSI \u0026lt; 0.5) are indicated. The arrows for each axis indicate mean values of corresponding shaded units. The histogram in the upper right shows the distribution of differences of orientation tuning widths between two opposite directions (preferred and non-preferred directions). For direction-selective (but not non-direction-selective) cells, orientation tuning curves for non-preferred directions tend to be narrower than those for preferred directions. (B) Data from 110 cells in layer 4. The same format is used as in A. (C) Data from 47 cells in layer 5. The same format is used as in A. (D) Data from 98 cells in layer 6. The same format is used as in A. Note that the amount of decrease in orientation tuning width in the non-preferred direction differs according to cortical layer. It increases gradually in the direction of visual information flow (layer 4 ‚Üí layer 2/3 ‚Üí layer 5 ‚Üí layer 6). ","date":"1 May 2016","permalink":"/publications/ejn_2016/","section":"Selected publications","summary":"Taekjun Kim, Ralph D Freeman \u003ccode\u003eEuropean Journal of Neuroscience\u003c/code\u003e","title":"Direction selectivity of neurons in the visual cortex is non‚Äêlinear and lamina‚Äêdependent"},{"content":"","date":null,"permalink":"/tags/histology/","section":"Tags","summary":"","title":"Histology"},{"content":"","date":null,"permalink":"/tags/modeling/","section":"Tags","summary":"","title":"Modeling"},{"content":"Article info # Authors Kayeon Kim, Taekjun Kim, Taehwan Yoon, Choongkil Lee Publication date 2015/12/15 Journal PLOS One DOI https://doi.org/10.1371/journal.pone.0144929 Abstract #A focal visual stimulus outside the classical receptive field (RF) of a V1 neuron does not evoke a spike response by itself, and yet evokes robust changes in the local field potential (LFP). This subthreshold LFP provides a unique opportunity to investigate how changes induced by surround stimulation leads to modulation of spike activity. In the current study, two identical Gabor stimuli were sequentially presented with a variable stimulus onset asynchrony (SOA) ranging from 0 to 100 ms: the first (S1) outside the RF and the second (S2) over the RF of primary visual cortex neurons, while trained monkeys performed a fixation task. This focal and asynchronous stimulation of the RF surround enabled us to analyze the modulation of S2-evoked spike activity and covariation between spike and LFP modulation across SOA. In this condition, the modulation of S2-evoked spike response was dominantly facilitative and was correlated with the change in LFP amplitude, which was pronounced for the cells recorded in the upper cortical layers. The time course of covariation between the SOA-dependent spike modulation and LFP amplitude suggested that the subthreshold LFP evoked by the S1 can predict the magnitude of upcoming spike modulation.\nFigures #Fig1. Trial structure for S1-S2 sequence stimuli # A white cross indicates central fixation and a dashed white circle (invisible to the animal) represents the boundary of the classical receptive field (RF). The stimulus onset asynchrony (SOA) between S1 and S2 varied in steps of 10 ms. In some trials, only S1 or S2 was presented. The SOA of 0 ms corresponds to simultaneous presentation of S1 and S2. After completion of S1-S2 presentation, a saccade target was presented at one of four randomly-chosen locations, up, down, left, and right with respect to the fixation target. These target positions never were in the RF. Fig2. Properties of subthreshold (sLFP) # A. Spatial layout of stimulus configuration: the cross marks the central fixation target; dashed white circle represents the RF (1.6¬∞ in diameter, centered at 0.6¬∞ right and 4.1¬∞ down); a Gabor stimulus at the RF and three identical stimuli positioned outside the RF spaced at intervals of one RF diameter away from the RF along the direction collinear to preferred orientation (a, b, and c); the calibration bar indicates 1¬∞. The distance of stimuli at a, b, and c from the RF center in cortical dimension was estimated to be 3.28, 5.89, and 8.06 mm, respectively. B, C. Raster and spike density plots (B) and mean LFP traces (C). Shading in C indicates ¬±2 SE. From top to bottom, responses to the Gabor stimulus at the RF alone, and those at a, b, and c alone are shown, aligned at the stimulus onset times (vertical lines). Note that a robust LFP change was evoked by the stimuli at a, b, and c, while the spike activity remained unchanged. The dominant positive and negative peaks of this sLFP are indicated as ‚Äòp‚Äô and ‚Äòn‚Äô, respectively. D. Power (RMS) of sLFP as a function of cortical distance between the center of Gabor stimulus in the RF surround and the center of RF, extracted from data on 56 surround stimuli tested at 40 cortical sites. The curve is a fitted function in the form of , following the inverse distance law of sound pressure, where x is cortical distance in mm between the center of Gabor stimulus in the RF surround and the center of RF. Parameter A was estimated to be 0.21, and its 95% confidence limits were 0.10 and 0.31; B was estimated to be 0.00. E. The amplitude of the sLFP, as measured from positive to negative peaks (as shown in C) as a function of cortical distance between the center of Gabor stimulus in the RF surround and the center of RF for 52 conditions. Four of the 56 conditions in D, for which the peaks could not be determined, were excluded. The curve is a fitted function in the same form as in D. Parameter A was estimated to be 0.67, its 95% confidence limits were 0.27 and 1.07; B was estimated to be 0.02. F, G. Latency to positive (open squares) and negative (dots) peaks of sLFP as a function of cortical distance between the center of Gabor stimulus in the RF surround and that at the center of RF, separately for monkey IR (F) and monkey CR (G) from the 52 conditions shown in E. The data were separately fitted with linear regression equations: y = 4.83x+54.79 and y = 3.90x+134.46 for monkey IR, and y = 3.27x+73.90 and y = 2.71x+147.44 for monkey CR, for latency to the positive and negative peak, respectively, where x is cortical distance between the two stimuli (p\u0026lt;0.05 for all cases). Fig3. Spike and LFP activity of a representative cell # A. Spatial layout of stimulus configuration: the cross marks the fixation target; the dashed white circle(invisible to the animal) shows the boundary of the RF; the calibration bar indicates 1¬∞. The RF was centered 3.3¬∞ right and 4.2¬∞ down. Two Gabor stimuli, one at the RF (S2), and the other in the RF surround (S1) are shown. B, C. Raster and spike density plots (B) and mean LFP traces (C) in response to S2 alone (upper) and S1 alone (lower), aligned at their onset times (dashed vertical line). Shadings in C indicate ¬±2 SE. Y-axis indicates spike density in spikes/s in B, and LFP amplitude in mV in C. Note that a robust LFP change was recorded in response to S1 alone, while the cell did not discharge spikes. D. SOA time plot for response modulation during trials with S1-S2 sequence stimuli, showing spike activity as a function of SOA and time, aligned at S2 onset. Activity is coded by color, as indicated by the calibration bar at top. White dots indicate the time of S1 onset for each SOA condition. The spike density for the S2-alone condition is given in a separate color map at bottom for comparison. Note that depending on SOA, spike density varied considerably in terms of magnitude and time course. Spike density for an SOA of 70 ms is indicated by the blue horizontal line, whereas the reference density for S2 alone (at bottom) is indicated by the horizontal black line; the time courses of both are shown in the upper panel of E with the same color coding. E, F. Upper: Spike (E) and LFP activity (F) in response to S1-S2 sequence stimuli with an SOA of 70 ms (blue), aligned at the time of S2 onset (dashed vertical lines). In each panel, a black trace indicates the reference of the S2-alone condition. Shadings indicate ¬±2 SE. Lower: The magnitude of modulation (S1-S2 sequence minus S2-alone) in firing rate (E) or LFP (F) is plotted for the SOA of 70 ms. Fig4. Spike and LFP activity of another representative cell # Same conventions as Fig 3. A. The RF was centered 0.6¬∞ right and 4.0¬∞ down. B, C. Raster and spike density plots (B) and mean LFP traces (C) in response to S2 alone (upper) and S1 alone (lower) aligned at their onset times. Shadings indicate ¬±2 SE. D. SOA time plot. Representative spike densities for SOA of 10 and 40 ms are indicated by red and blue horizontal lines, respectively. E, F. Upper and middle panels: Spike (E) and LFP activity (F) in response to S1-S2 sequences with SOAs of 10 (red) and 40 ms (blue); black traces indicate references taken from the S2-alone condition. Bottom panels: The magnitude of modulation (S1-S2 sequence minus S2-alone condition) in spike (E) and LFP (F) are plotted for SOAs of 10 ms (red) 40 ms (blue). Note that for the SOA of 10 ms, the magnitude of modulation in spike activity was negative (suppressed) and the modulation of LFP was relatively weak, whereas for the SOA of 40 ms, the magnitude of modulation in spike activity was positive (facilitated) and the modulation of LFP was relatively strong. Fig5. Effects of S1 on spike (A) and LFP (B) response across 11 SOA conditions # Each colored symbol represents the mean magnitude of spike (A) or LFP (B) response in percentage with respect to S2-alone condition for corresponding SOA condition of each of 30 cells for which nearest S1 was tested. Black symbols represent median values of those means with 1SEs. Percent modulations less than 100 indicate suppression and those larger than 100 indicate facilitation by addition of S1. Fig6. Deviation of LFP from linear sum in the representative cell of Fig 3 # A. Shown are LFP traces in an arbitrary unit for each SOA condition derived by the mean LFP traces observed during S1-S2 sequence stimulation minus the SOA-adjusted linear sum of S1-evoked LFP and S2-evoked LFP. B. Deviation of LFP in RMS power. Each colored symbol represents the mean deviation across SOA conditions for each of 30 cells shown in Fig 5. Black symbols represent their mean values with 1SEs. Fig7. Relationship between spike and LFP modulation # A. Scatter plot showing the percentage changes in spike activity and RMS LFP power in the S1-S2 sequence relative to the S2-alone condition for 330 stimulus conditions (11 SOA conditions X 30 sites). They are positively related as indicated by the Pearson correlation coefficient and its p-value inside the panel. B. LFP (left) and corresponding spike density (right) traces from all 62 sites in which S2 alone was tested, averaged for five depth groups divided into depth segments of 300Œºm, measured from the surface of the dura. The deepest trace (bottom) includes all recording sites below 1800Œºm from the dura. The shading represents ¬±1 SE. C, D. Relationship between spike and LFP modulation subdivided into two depth groups, upper (C, \u0026lt;1.2mm) and lower (D, \u0026gt;1.2mm). Same convention as A. Fig8. Correlation between spike activity and LFP # A. SOA-dependent spike modulation for the cell shown in Fig 3. The mean firing rates during the post-stimulus period of 50‚Äì150 ms of S2 are plotted as a function of SOA. Vertical dashed lines are the reference response levels evoked by S2 alone. B. Simultaneously recorded mean LFP traces in an arbitrary unit for corresponding SOAs for the cell shown in A. Traces are vertically shifted for visibility. C. Time course of mean correlation between spike and LFP modulation. The correlation coefficient between the SOA-dependent firing rate (as shown in A) and the instantaneous amplitude of LFP (as shown in B) was first calculated every 1 ms for each condition. Shown is the mean correlation coefficient time course averaged over all 517 stimulus conditions (11 SOAs X 47 S1-S2 sequences) from 31 cells including cases in which S1 was tested at more than one RF diameter away. The shading represents ¬±1 SE. Note a positive correlation immediately after S2 onset (arrow) and a subsequent negative correlation. D, E. Frequency histograms of the time from S2 onset (D) and the correlation coefficient (E) for the 1st (upper) and 2nd (lower) peaks in the time course of correlation. Dashed vertical lines indicate distribution means. For the 1st peak correlation, the mean location was 45.25 ¬±36.0 ms and the mean correlation coefficient was 0.32 ¬±0.24. For the 2nd peak, the mean location was 119.32 ¬±38.3 ms, and the mean correlation coefficient was -0.42 ¬±0.33. Black bars indicate significant cases, as determined with a bootstrap statistical test (p\u0026lt;0.05). ","date":"15 December 2015","permalink":"/publications/plos_one_2015/","section":"Selected publications","summary":"Kayeon Kim, Taekjun Kim, Taehwan Yoon, Choongkil Lee \u003ccode\u003ePLOS One\u003c/code\u003e","title":"Covariation between Spike and LFP Modulations Revealed with Focal and Asynchronous Stimulation of Receptive Field Surround in Monkey Primary Visual Cortex"},{"content":"","date":null,"permalink":"/tags/lfp/","section":"Tags","summary":"","title":"LFP"},{"content":"","date":null,"permalink":"/tags/receptive-field/","section":"Tags","summary":"","title":"Receptive Field"},{"content":"","date":null,"permalink":"/tags/spatiotemporal/","section":"Tags","summary":"","title":"Spatiotemporal"},{"content":"","date":null,"permalink":"/tags/response-selectivity/","section":"Tags","summary":"","title":"Response Selectivity"},{"content":"","date":null,"permalink":"/tags/stimulation/","section":"Tags","summary":"","title":"Stimulation"},{"content":"","date":null,"permalink":"/tags/tms/","section":"Tags","summary":"","title":"TMS"},{"content":"Article info # Authors Taekjun Kim, Elena A Allen, Brian N Pasley, Ralph D Freeman Publication date 2015/05/01 Journal Brain stimulation DOI https://doi.org/10.1016/j.brs.2015.01.407 Abstract # Background Transcranial magnetic stimulation (TMS) is used to selectively alter neuronal activity of specific regions in the cerebral cortex. TMS is reported to induce either transient disruption or enhancement of different neural functions. However, its effects on tuning properties of sensory neurons have not been studied quantitatively. Objective/hypothesis Here, we use specific TMS application parameters to determine how they may alter tuning characteristics (orientation, spatial frequency, and contrast sensitivity) of single neurons in the cat\u0026rsquo;s visual cortex. Methods Single unit spikes were recorded with tungsten microelectrodes from the visual cortex of anesthetized and paralyzed cats (12 males). Repetitive TMS (4 Hz, 4 s) was delivered with a 70 mm figure-8 coil. We quantified basic tuning parameters of individual neurons for each pre- and post-TMS condition. The statistical significance of changes for each tuning parameter between the two conditions was evaluated with a Wilcoxon signed-rank test. Results We generally find long-lasting suppression which persists well beyond the stimulation period. Pre- and post-TMS orientation tuning curves show constant peak values. However, strong suppression at non-preferred orientations tends to narrow the widths of tuning curves. Spatial frequency tuning exhibits an asymmetric change in overall shape, which results in an emphasis on higher frequencies. Contrast tuning curves show nonlinear changes consistent with a gain control mechanism. Conclusions These findings suggest that TMS causes extended interruption of the balance between sub-cortical and intra-cortical inputs. Figures #Fig1. Experimental paradigm # A. Figure-8 coil is positioned obliquely near the transverse plane superior to the visual cortex (1.52cm apart from the cortical surface). Its midpoint is aligned to the left visual cortex craniotomy (Horsley-Clarke coordinates P4 L2). Tungsten electrode penetration is made at an angle of A45, M0. B. We examine how rTMS alters selectivity of cells in the visual cortex. To do this, we measure orientation, spatial frequency, and contrast tuning properties of cells and compare the properties between pre- and post-TMS conditions. For orientation tuning run, 710 differently oriented circular grating patches (stimulus duration: 2 seconds, inter-stimulus interval: 2 seconds) are presented in a cell‚Äôs classical receptive field in each trial (depicted as squares below the time arrow). 4Hz TMS pulse train is delivered for 4 seconds in the inter-trial interval (10 seconds) between 15th and 16th trials. Black and gray colors are used to represent pre- and post-TMS conditions, respectively. Fig2. Three examples showing TMS effects on orientation selectivity # A. Neural response of an example cell is depicted as a form of matrix. X- and Y-axis indicate trial number and orientation, respectively. In each trial (column), 10 different orientations (40130¬∞, 10¬∞ step) were tested. Response magnitude is coded with colors on a blue-red scale. 4Hz rTMS (downward arrow) was delivered for 4 seconds just before the 16th trial. The total 40 trials are divided into three groups based on elapsed time from TMS delivery: pre-TMS (115th trials, black filled circles and line), post-TMS (1630th trials, gray filled circles and line), and recovery (3140th trials, open circles and dotted line) conditions. Trial number was translated into time from TMS delivery. Neural response was abruptly changed as soon as TMS was applied. The TMS effect was reversible and it lasted for approximately 10 minutes. B. Three orientation tuning curves were created from the cell depicted in (A). Black, gray, and open circles are mean neural responses for 10 different orientations computed in pre-TMS, post-TMS, and recovery conditions, respectively. Error bar indicates standard error of mean. Error bars are smaller than circles when not visible. In these three conditions, the preferred orientation is not changed but response magnitude is clearly diminished in post-TMS condition compared with the other two conditions. More detailed analysis of TMS-effects on orientation selectivity will be dealt in Figure 5A~D. C, D. Another example cell showing TMS effects on orientation selectivity. The same conventions are used as in (A), (B). The black circle at 180¬∞ is hidden by the open one. E, F. Our rTMS (4Hz, 4sec) mainly caused prolonged suppression of neural responses in cat‚Äôs visual cortex (80 out of 92 units). But, in small cases, TMS-induced facilitation was also observable. One example is introduced here. Fig3. Three examples showing TMS effects on spatial frequency selectivity # A. In this example cell, neural responses for 7 different spatial frequencies (0.131.2 c/deg, evenly distributed in logarithmic scale) were recorded in 40 trials. The plotting conventions are the same as in Figure 2A. 4Hz rTMS (downward arrow) was applied just before the 16th trial. The total 40 trials are divided into three groups to create tuning functions in pre-TMS (115th trials, black filled circles and line), post-TMS (1630th trials, gray filled circles and line), and recovery (3140th trials, open circles and dotted line) conditions. B. Spatial frequency tuning curves of pre-TMS, post-TMS and recovery conditions were created from the cell depicted in (A). Equation used for curve fitting is the same as the one used for orientation tuning. But spatial frequency tuning curve is not Gaussian shaped, because x-axis is transformed to logarithmic scale. The area under the gray curve (post-TMS condition) is smaller than those under the other two curves. Note that TMS-induced suppression is more apparent in low spatial frequency range than high spatial frequency range. More detailed analysis of TMS-effects on spatial frequency selectivity will be dealt in Figure 5E~H. C, D. Another example cell showing TMS effects on spatial frequency selectivity. E, F. This example shows TMS-induced facilitation in spatial frequency tuning run. Fig4. Three examples showing TMS effects on contrast selectivity # A. In this example cell, neural responses for 10 different contrast values (5100%, evenly distributed in logarithmic scale) were recorded in 40 trials. The plotting conventions are the same as in Figure 2A. 4Hz rTMS (downward arrow) was applied just before the 16th trial. The total 40 trials are divided into three groups to create tuning functions in pre-TMS (115th trials, black filled circles and line), post-TMS (1630th trials, gray filled circles and line), and recovery (3140th trials, open circles and dotted line) conditions. B. Contrast tuning curves of pre-TMS, post-TMS and recovery conditions were created from the cell depicted in (A). The curves are fitted with the Naka-Rushton function. The area under gray curve (post-TMS condition) is smaller than those of the other two curves. Note that despite of TMS-induced suppression, response magnitude at the lowest (5%) and highest (100%) contrast is comparable in pre- and post-TMS condition. More detailed analysis of TMS-effects on contrast selectivity will be dealt in Figure 5I~L. C, D. Another example cell showing TMS effects on contrast selectivity. E. F. This example shows TMS-induced facilitation in contrast tuning run. Neural response for recovery condition was not recorded, so the matrix in (A) is blank between 31st and 40th trials. Fig5. Summary of TMS effects on response selectivity # A, B, C, D. TMS effects on orientation selectivity were tested in 35 cells. (A) Three parameters (K, Œº, œÉ) representing the maximum neural response, preferred orientation, tuning width are taken from Gaussian fitting function. (B) Parameter K is compared between pre- and post-TMS conditions. Difference between two conditions is significant (Wilcoxon signed-rank test, p\u0026lt;0.001). TMS-induced suppression and facilitation cases are indicated by filled triangles and open squares, respectively. (C) Parameter Œº is compared between pre- and post-TMS conditions. Regardless of suppression or facilitation, the preferred orientation is not affected by TMS. (D) Parameter œÉ is compared between pre- and post-TMS conditions. TMS-induced suppression often makes orientation tuning sharper, and this change (reduced tuning width) is statistically significant (Wilcoxon signed-rank test, p\u0026lt;0.01). E, F, G, H. TMS effects on spatial frequency selectivity were tested in 32 single units. (E) Parameter K represents the maximum neural response. Low and high cutoff spatial frequencies were defined as the lower and higher spatial frequencies evoking the half-maximum neural response, respectively. (F) Parameter K is significantly decreased in post-TMS condition (Wilcoxon signed-rank test, p\u0026lt;0.001). (G) Low cutoff spatial frequency is significantly higher in post-TMS than pre-TMS conditions (Wilcoxon signed-rank test, p\u0026lt;0.01). (H) High cutoff spatial frequency is not significantly changed between pre- and post-TMS conditions. I, J, K, L. TMS effects on contrast selectivity were tested in 25 single units. (I) Three parameters (Rmax, n, c50), taken from Naka-Rushton function, represent the saturated neural response, steepness of curve, stimulus contrast producing the half-maximum neural response, respectively. (J) Parameter Rmax is not significantly changed by TMS (Wilcoxon signed-rank test, p=0.80). (K) Parameter n is compared between pre- and post-TMS conditions. (L) Parameter c50 is significantly increased in post-TMS condition (Wilcoxon signed-rank test, p\u0026lt;0.01). Considering (J) and (L) together, results suggest that TMS effects on neural response are better explained by contrast-gain rather than response gain control. ","date":"1 May 2015","permalink":"/publications/brainstimul_2015/","section":"Selected publications","summary":"Taekjun Kim, Elena A Allen, Brian N Pasley, Ralph D Freeman \u003ccode\u003eBrain stimulation\u003c/code\u003e","title":"Transcranial magnetic stimulation changes response selectivity of neurons in the visual cortex"},{"content":"","date":null,"permalink":"/tags/neural-connections/","section":"Tags","summary":"","title":"Neural Connections"},{"content":"Article info # Authors Taekjun Kim, Ralph D Freeman Publication date 2014/08/22 Journal Neuroscience DOI https://doi.org/10.1016/j.neuroscience.2014.05.041 Abstract #Organization of the central visual pathway is generally studied from a perspective of feedforward processes. However, there are horizontal connections and also strong feedback from extra striate to visual cortex. Here, we use visual stimuli designed to maximize relative differential involvements of these three main types of connections. The approach relies on differences between stimulation within the classical receptive field (CRF) and that of the surround region. Although previous studies have used similar approaches, they were limited primarily to spatial segregation of neural connections. Our experimental design provides clear segregation of fast and slow components of surround modulation. We assume these are mediated by feedback and horizontal connections, respectively, but other factors may be involved. Our results imply that both horizontal and feedback connections contribute to integration of visual information outside the CRF and provide suppressive or facilitative modulation. For a given cell, modulation may change in strength and sign from suppression to facilitation or the reverse depending on surround parameters. Sub-threshold input from the CRF surround increases local field potential (LFP) power in distinct frequency ranges which differ for suppression and facilitation. Horizontal connections have delayed CRF-surround modulation and are sensitive to position changes in the surround. Therefore, surround information beyond the CRF is initially processed by fast connections which we consider to be feedback, whereas spatially tuned mechanisms are relatively slow and presumably mediated by horizontal connections. Overall, results suggest that convergent fast (feedforward) inputs determine size and structure of the CRFs of recipient cells in visual cortex. And fast connections from extra striate regions (feedback) plus slow-tuned connections (horizontal) within visual cortex contribute to spatial influences of CRF surround activation.\nFigures #Fig1. Visual stimulus design # Two sets of center-surround (CRF-outside CRF) stimuli: annulus surround (left) and small patch surround (right). With both sets, distance between center and surround stimuli is systematically varied. In annulus surround pattern (A), annuli of different widths are used so that increments of center-surround distances are accompanied with decreases of total stimulated area. In small patch surround pattern (B), change of center-surround distance does not cause increase or decrease of total area stimulated. (C) Sequence of a trial. Optimal sinusoidal moving gratings are used to stimulate CRF and surround regions of a cell under study. The center (CRF) stimulus (2000-ms duration, 50% contrast) is presented first, followed by surround stimuli (500-ms duration, 100% contrast) after a 500-ms temporal interval. (D) Small-patch surround stimuli and annulus surround are tested in separate blocks. For small patch surround blocks, two-patch surround stimuli are positioned symmetrically with respect to the center stimulus along the axis of preferred orientation. Inter-patch distances (white arrows) are chosen randomly as one out of four values (0.5‚Äì3.5 deg, 1 deg step) for each trial. For annulus surround blocks, the outer diameter of the annulus is fixed at 30 deg. Therefore, four levels of center-surround distance are controlled by the inner diameter of the annulus. In addition to four ‚Äúcenter + surround‚Äù and four ‚Äúsurround-alone‚Äù conditions, a ‚Äúcenter-alone‚Äù presentation is tested as a control. Fig2. A representative cell showing suppressive center-surround modulation # (A) Dashed and solid curves are spike density functions computed for ‚Äúcenter-alone‚Äù and ‚Äúcenter + surround (dist1)‚Äù conditions, respectively. Time 0 indicates onset of surround stimulus whose duration is 500 ms. Onset and offset of surround stimulus are indicated by two downward arrows. Note that the magnitude of the solid curve is lower than that of the dashed only from the 0 to 500-ms interval, demonstrating that the neural response to the center stimulus is suppressed by the surround. (B) For each of 9 stimulus conditions, mean spike count during the 0‚Äì500 ms period is computed and then normalized with the value computed for ‚Äúcenter-alone‚Äù condition. The smaller numbers for the x-axis represent the nearer center-surround distances. In this case, strength of surround suppression gets weaker as center-surround distance increases. (C) Each curve is created by subtracting the spike density function for ‚Äúcenter-alone‚Äù condition from that of each ‚Äúcenter + surround‚Äù condition. Nearer center-surround distances are depicted in darker shades. For efficient comparisons between the four distances, curves are truncated to the interval from 0 to 500 ms. Fig3. An example cell which exhibits facilitative center-surround modulation # The same conventions are used as in Fig. 2. (A) Spike density functions computed for ‚Äúcenter-alone (dashed)‚Äù and ‚Äúcenter + dist1 (solid)‚Äù conditions. Arrows indicate times at which the surround is presented (first arrow) and when it is turned off (second arrow). (B) Normalized responses for nine stimulus conditions. Strength of surround facilitation becomes weaker as center-surround distance is increased. (C) Time course of surround modulation. Surround facilitation tends to be diminished and delayed as surround distance from the center (CRF) is increased. Fig4. Proportions of significant modulation for annulus and small patch surround conditions # Proportions of significant modulation for annulus and small patch surround conditions (Two-sided Mann‚ÄìWhitney U-test (p \u0026lt; 0.05)). Although, for both conditions, proportions of significant modulation cases (filled and unfilled bar areas) decreases as center-surround distance increases, bar heights for annulus conditions are nearly twice as tall as those for small patch application at corresponding center-surround distances. This demonstrates that the annulus surround is more effective for the induction of significant surround modulation. In addition, the dominant sign of surround modulation is suppression for the annulus pattern, but it is facilitation for the small patch. Furthermore, for the annulus pattern, relative ratios of suppression (filled circles) diminish with increasing center-surround distance. This suggests that suppression requires stronger surround input than facilitation. Fig5. Modulation strength comparison: annulus vs. small patch pattern # Modulation strength comparison: annulus vs. small patch pattern. Each circular symbol (N = 92) represents the mean value of normalized responses for four ‚Äúcenter + surround‚Äù conditions (e.g., 1‚Äì4th bar in Fig. 3B). Abscissa values are for annulus conditions and ordinate levels are for small patch trials. Shading of circles convey statistical significance (two-sided Mann‚ÄìWhitney U-test (p \u0026lt; 0.05)) of center-surround modulation (open circles: not significant for either condition, gray filled: significant for only one condition. black filled: significant for both conditions). Symbols in left half of the graph mean that suppressive modulation is induced by annulus surround pattern. Almost all symbols in left half of the graph are positioned above the diagonal line (55 vs. 9). This means that neural responses to small patch surround patterns are stronger than those for the annulus. This follows because of surround facilitation (in 2nd quadrant, top left) or weakened surround suppression (in 3rd quadrant, bottom left). Symbols in the right half (facilitation cases for the annulus pattern) are positioned mainly in the 1st quadrant (top right), and rarely in the 4th quadrant (bottom right). Within the 1st quadrant, symbols are evenly distributed with respect to the diagonal line (9 vs. 9). This means that surround facilitation induced by the small patch surround can be either weaker or stronger than that caused by the annulus. These results support the idea that suppressive modulation requires stronger surround input than that for facilitation (see details in text). Fig6. Population average z-scored LFP spectrograms # Population average z-scored LFP spectrograms. For a 10‚Äì100-Hz frequency range, LFP power change from the baseline is plotted as a function of time. (Left two columns) Annulus surround pattern, (Right two columns) Small patch surround pattern. (A‚ÄìH) To exclude effects of spiking activity on LFP spectrograms, 38 tests are used for which the ‚Äúsurround-alone (dist2)‚Äù condition, for both surround patterns, does not evoke spiking activity. For the ‚Äúsurround-alone‚Äù condition, the annulus causes a larger change in LFP power than that for the small patch (A vs. C), and the main change is focused on the high gamma frequency range (approximately 60‚Äì80 Hz). This 60‚Äì80-Hz frequency-specific change in annulus surround-alone result is also revealed in F, reflecting center-surround modulation of the LFP spectrogram. (I‚ÄìP) Z-scored LFP spectrogram comparisons: surround suppression vs. surround facilitation. For each surround pattern, population data are divided into two groups: suppression vs. facilitation. Again, the tests included in this analysis do not evoke spiking responses for the ‚Äúsurround-alone‚Äù condition so they are distinguishable only at subthreshold levels (I, J, K, and L). Note for the annulus surround, that increased LFP power in the 60‚Äì80 Hz range (as shown in Fig. 6A) is clear for suppression (I), but not for facilitation (J). Regardless of surround type, facilitation cases of the ‚Äúsurround-alone‚Äù conditions (J, L) are similar in that LFP power change for the low-frequency range is bigger than that for high frequencies. Depending on sign of modulation, center-surround effects in LFP spectrograms show the largest differences in the 80‚Äì100-Hz frequency range (M vs. N or O vs. P). LFP power in this range decreases for suppression cases, but increases for facilitation. Fig7. Predictions for time course of surround modulation # The first and second downward arrows represent onset and offset of surround stimuli, respectively. Therefore, the center stimulus is presented earlier and lasts longer than that for the surround. Time courses depicted in darker shades indicate nearer center-surround distances. The predictions are based on the following assumptions. (1) Surround modulation is mediated by both feedback and horizontal connections. (2) There is limited interaction between the two types of neural connections. (3) Given that conduction velocities of feedback connections are much faster than those for the horizontal type, the earliest part of surround modulation is mediated by feedback connections. (4) The onset of the feedback component of surround modulation is minimally affected by center-surround distance. (5) The onset of the horizontal component of surround modulation is increasingly delayed as center-surround distance increases. (A) Annulus surround pattern: Increasing center-surround distance causes decrease in both feedback and horizontal components of surround modulation. So, differences between middle and far conditions occur at the same time as those between near and far. (B) Small patch surround pattern: increasing center-surround distance causes selective decrease of horizontal component of surround modulation. So, differences between middle and far conditions are delayed more than those between near and far. Fig8. Time course of surround modulation # Time course of surround modulation: suppression (A, B: annulus surround/C, D: small patch surround), facilitation (E, F: annulus surround/G, H: small patch surround). Two downward arrows indicate onset and offset of surround stimuli, respectively. Nearer center-surround distances are depicted in darker shades. (A, C, E, G): differences between ‚Äúcenter-alone‚Äù control and each of four ‚Äúcenter + surround‚Äù conditions are z-score normalized using mean and standard deviation of differences during baseline periods (from ‚àí500 to 0 ms before surround stimulus onset). The dashed line serves as a reference for comparison among conditions. (B, D, F, H): dist4 curve is subtracted from each of four gray curves. The positions of three triangles indicate onset times of significant difference (two-sided Wilcoxon signed rank test in the 100-ms sliding window, p \u0026lt; 0.05) between each of dist1‚Äì3 curves and control (dist4). In H, difference between dist3 and 4 is not statistically significant, so only two triangles are drawn. For annulus surround pattern (B, F), differences between the resultant three curves appear from the initial part of the modulation without substantial difference in onset delay depending on center-surround distance. However, for the small patch surround pattern (D, H), difference between dist2 or 3 and dist4 appear later than those between dist1 and 4. ","date":"22 August 2014","permalink":"/publications/neurosci_2014/","section":"Selected publications","summary":"Taekjun Kim, Ralph D Freeman \u003ccode\u003eNeuroscience\u003c/code\u003e","title":"Selective stimulation of neurons in visual cortex enables segregation of slow and fast connections"},{"content":"","date":null,"permalink":"/tags/surround-modulation/","section":"Tags","summary":"","title":"Surround Modulation"},{"content":"Article info # Authors Taekjun Kim, HyungGoo R Kim, Kayeon Kim, Choongkil Lee Publication date 2012/10/16 Journal PLOS One DOI https://doi.org/10.1371/journal.pone.0047543 Abstract #The spike activity of single neurons of the primary visual cortex (V1) becomes more selective and reliable in response to wide-field natural scenes compared to smaller stimuli confined to the classical receptive field (RF). However, it is largely unknown what aspects of natural scenes increase the selectivity of V1 neurons. One hypothesis is that modulation by surround interaction is highly sensitive to small changes in spatiotemporal aspects of RF surround. Such a fine-tuned modulation would enable single neurons to hold information about spatiotemporal sequences of oriented stimuli, which extends the role of V1 neurons as a simple spatiotemporal filter confined to the RF. In the current study, we examined the hypothesis in the V1 of awake behaving monkeys, by testing whether the spike response of single V1 neurons is modulated by temporal interval of spatiotemporal stimulus sequence encompassing inside and outside the RF. We used two identical Gabor stimuli that were sequentially presented with a variable stimulus onset asynchrony (SOA): the preceding one (S1) outside the RF and the following one (S2) in the RF. This stimulus configuration enabled us to examine the spatiotemporal selectivity of response modulation from a focal surround region. Although S1 alone did not evoke spike responses, visual response to S2 was modulated for SOA in the range of tens of milliseconds. These results suggest that V1 neurons participate in processing spatiotemporal sequences of oriented stimuli extending outside the RF.\nFigures #Fig1. An image volume # A: Spatiotemporal volume of an exemplary visual world. Each rectangle represents a topographically organized unit space corresponding to known receptive field of a single neuron of central visual system such as V1. B: Bars represent oriented line segment of simplified contours of visual events such as a swinging bat at instantaneous moments, t1 and t2. Fig2. Trial paradigm # (A) A spatial layout of stimulus condition. A white cross indicates central fixation and the dashed white circle (invisible to the animal) represents the classical receptive field (RF). While the eye position was maintained within a window of 1 deg in radius centered about the fixation point, a static Gabor stimulus, S1, was first presented outside RF, and a second static Gabor stimulus, S2, was presented within RF. Both were presented for 20 ms each with a varying stimulus onset asynchrony (SOA), ranging from 0 to 100 ms. The animals‚Äô task was to maintain central fixation and make a saccade following the target for liquid reward. (B) Temporal sequence of a trial. Table1. Summary of stimulus conditions # Fig3. Response of a representative cell. # (A) Spatial relation between stimuli in screen coordinates (calibration bar‚Ää=‚Ää1 deg). White cross represents fixation target, and the dashed circle (invisible to the animal) encloses the RF of the recorded neuron determined with a spatial summation test. Gabor stimulus at RF (S2) is at preferrred orientation. S1 was presented at one of four locations, a‚Äìd, along the axis orthogonal to that of RF orientation, with a spacing of one RF diameter. All S1 orientations were parallel to S2. There were 44 unique stimulus sequences (4 S1 positions√ó11 SOAs), plus five single stimulus conditions at each S1 and S2 locations. These 49 stimulus conditions were randomly repeated. (B) Raster and density plots of response to S1 at positions a‚Äìd aligned at its onset. Spike density function was derived by convolving spike sequence with an asymmetric kernel function [66]. Y-axis indicates spike density in spikes/s. Note that no S1 alone at positions a-d evoked spike response. (C) Raster and density plots for S2 alone and S1c-S2 sequence stimuli with SOAs of 30 and 50 ms chosen to illustrate response modulation. Trials are aligned at S2 onset. It can be seen that the magnitude of initial and sustained response varied with SOA. (D) An example SOA-time plot compiled from spike density for S1c-S2 sequence stimuli, the first stimulus at positions c and the second stimulus at RF. Y-axis is SOA, determined in 10-ms step. The times of S1 onset for each SOA condition are indicated as small white circles. Data are linearly interpolated across SOA. The S2-alone condition is given above for comparison. Note that the cell‚Äôs response varied with SOA. (E) Determination of significant modulation. Spike density curves for S2 alone (black) and S1c-S2 sequence with SOA of 80 ms (green), along with horizotal marks (top) of temporal epochs associated with statistically significant decrease (blue) and increase (red) from S2 alone condition. (F) Time course of significant modulation of spike response by sequence stimuli as shown in E. Spike density following S1‚ÄìS2 sequence was compared with spike density following S2 for each of temporal epochs of 30 ms with a shift of 5 ms. The temporal epochs with a statistically-significant decrease in spike density as determined with Mann-Whitney U-test are shown in blue bars, and significant increase in red bars, centering on corresponding analysis windows, revealing the magnitude and time course of suppressive and facilitative effects of S1 that depend on S1 position and SOA. The dark symbols represent significant modulation at p\u0026lt;0.01, and the light ones are p\u0026lt;0.05. Fig4. Modulation window # The cell of Fig. 3 is reproduced. (A) Response to S2 alone with the duration of 63 ms for inital transient response indicated with red dotted lines. (B) The duration of suppression by S1 at the position c is indicated with two green dotted lines. (C, D) Modulation window formed by the two durations in A and B in corresponding colors. Time of S1 onset is shown in a white line, interpolated across SOA. The S2-alone condition is given above for comparison in C. Note that the range of significant suppression and facilitation agrees well with modulation window. Also note that modulation is variable depending on SOA within modulation window.\nFig5. Response of another representative cell # (A) For this cell, S1 was presented at one of three locations, a‚Äìc, as shown along the axis collinear to that of RF orientation, with a spacing of one RF diameter. Some S1s encroached on the hemifield contralateral to RF. S1 orientation was collinear to S2. There were 33 unique stimulus sequences (3 S1 positions√ó11 SOAs), plus four single stimulus conditions at each S1 and S2 locations. These 37 stimulus conditions were randomly repeated within a block. (B) Spike activity with stimulation of S1 alone at locations, a‚Äìc. The cell remained silent with S1 at all tested locations. (C) Spike activity with S2 stimulus alone in raster and density (upper) and color (lower) plots. (D) SOA-time plots in the same format as Fig. 3D, for S1 at locations, a‚Äìc, from top to bottom. Color map of activity is shown to the right. Note a periodic SOA-dependency of activity modulation (E) Time course of significant modulation of spike response by sequence stimuli in the same format as Fig. 3F for S1 at locations, a‚Äìc, from top to bottom. Note that the activity modulation by the S1 at all locations was suppressive at virtually all SOAs. All the stimulus conditions of Fig. 5 were randomized within the same block during data collection. Fig6. Periodic SOA-dependency of surround modulation # (A) Modulation of spike response in percentage as a function of SOA for three S1 locations of Fig. 5; green: a, blue: b, red: c. (B) Periodic component. The best fit linear trend was removed from the spline fit of modulation percentage in A for each S1 position to remove the monotonic component (detrend.m provided by the MATLAB). (C) The auto-correlogram (black) of the detrended modulation of S1a (green curve of B) was fit with a cosine-Gaussian function, (gray). (D) Hstogram of periodicity. Each case is the first non-zero peak of a cosine-Gaussian function, taken from 53 out of 276 stimulus conditions, for which the cosine-Gaussian fit explained more than 90% variance of auto-correlation curve. For three examples of B, R-squares are 0.99 (S1a), 0.94 (S1b), 0.99 (S1c). The mean periodicity of 53 stimulus conditions is 36.62 ms. (E) Cross-correlation between detrended green and blue curves of B. The time lag at the maximum cross-correlation is 20 ms. This lag reflects the distance between S1a (green) and S1b (blue). Given that cortical distance between S1a and S1b was 3.07 mm, the propagation speed in this example is estimated to be 0.15 m/s. (F) Histogram of propagation speed. Shown is propagation speed for each of 30 cases in which at least one S1 position passed the periodicity criteria of C. The mean distribution is 0.14 m/s without 3 outliers. The median is 0.11 m/s. Fig7. Frequency histograms of response index for each SOA condition from 208 collinear (A), 52 parallel (B). # The mean indices were 91.23¬±18.40 and 96.28¬±15.24, respectively. These means are significantly less than 100% (Wilcoxon signed-rank test, all p\u0026lt;10‚àí7). The proportions of significant suppression and facilitation (black bars) were 21.24 and 3.72% (A), and 9.44 and 5.07% (B), respectively. Note that suppression was more frequent than facilitation, for both collinear and parallel configurations, but this difference was larger for collinear condition. Fig8. Pattern of modulation for collinear (A‚ÄìD) and parallel (E‚ÄìH) S1 conditions # (A) Combined time course of SOA-dependent significant facilitation (p\u0026lt;0.05) from 2288 SOA conditions of 208 collinear S1 stimuli. Normalized frequency of significant epoch is color-coded according to the color map shown on the right. Out of 2288, 208 (9.09%) SOA conditions included more than one temporal epoch with significant facilitation. (B) Time course of significant suppression combined from the same collinear S1 configurations. In 611 of 2288 (26.70%) SOA conditions, more than one temporal epoch showed significant suppression. (C) Normalized marginal frequency of significant facilitation from A (red) and suppression from B (blue) against peristimulus time. (D) Normalized marginal frequency of significant facilitation (A, red) and suppression (B, blue) during the poststimulus time period from 0 to 300 ms against SOA. Normallized marginal frequency was derived from marginal sum divided by the number of data points. (E‚ÄìH) Similar plots as A‚ÄìD combined from 572 SOA conditions of 52 parallel configurations. Out of 572 SOA conditions, 96 (16.78%) and 71 (12.41%) SOA groups showed significant facilitation and suppression, respectively. Thus, suppression was relatively common with collinear S1, and the relative ratio of facilitative modulation was higher with the parallel configuration. This was true even after the distance between S1 and S2 was taken into account (by subdividing S1 configuration conditions into two distance groups, one or two RF diameter away from RF center). Note that suppressive modulation was concentrated at around 100 ms after S2 onset time (C, G), whereas facilitative modulation was relatively more dispersed and dominant after around 200 ms after S2 onset, especially in the parallel configuration. Also note that collinear S1 tended to suppress at short SOA and facilitate at long SOA (D), whereas this dissociation was relatively weak with parallel S1 (H). Fig9. Scatter plot of selectivity index and cortical distance # Each dot represents selectivity index (SI) for each stimulus condition and anatomical distance between the centers of S1 and S2 for that condition. Mariginal histograms are also shown. The cortical distance was estimated from the cortical magnification factor [31]. Data are combined single and multiple unit data obtained from 227 stimulus conditions (i.e., S1 positions) for 105 recording sites in 2 monkeys. Black dots and bars indicate significant SIs (69 of 227 cases, 30.40%, p\u0026lt;0.05), as evaluated with a bootstrapping method. The proportion was also consistent for single units alone (23 of 92 stimulus conditions, 25%) and multiple unit activitiy (46 of 135 stimulus conditions, 34.07%).The proportion of significant SI decreased with the cortical distance between S1 and S2.\nFig10. Effects of stimulus speed # This cell is the same as Fig. 5. Time courses of spike response of the cell for S1 at one RF diameter away from S2 (A) and two RF diameters away (B) conditions. Shown in each panel are mean spike density traces for S1‚ÄìS2 sequence with SOA of 50 ms (blue) and 100 ms (red). Mean spike density for the S2-alone condition is also shown for comparison (black) with its 95% confidence interval (mean¬±2 SEM, gray shade). All these stimulus conditions, including other SOA conditions, were randomized within the same block during data collection. Note that the peak spike response to S1a‚ÄìS2 sequence with SOA of 50 ms (blue trace in A) was reduced by half compared to response to S2 alone (black). Also note that the magnitude of this response is quite different from that for S1b‚ÄìS2 sequence with SOA of 100 ms (red trace in B), although the apparent motion speed of these two conditions is roughly the same.\n","date":"16 October 2012","permalink":"/publications/plos_one_2012/","section":"Selected publications","summary":"Taekjun Kim, HyungGoo R Kim, Kayeon Kim, Choongkil Lee \u003ccode\u003ePLOS One\u003c/code\u003e","title":"Modulation of V1 spike response by temporal interval of spatiotemporal stimulus sequence"},{"content":"Taekjun Kim, Ph.D. # Download CV Acting Assistant Professor Department of Neurobiology and Biophysics, Washington National Primate Research Center, University of Washington Professional Summary # Ph.D. in Vision Science (Neuroscience field) 15+ years research experience applying statistical and machine learning methods to analyze neural and behavioral data Demonstrated productivity in research, with 13 peer-reviewed, including 9 as the first author publications Skilled in various methodologies, including electrophysiology, computational modeling, optical imaging, human psychophysics Proficiency in programming languages - Python, Matlab, SQL Strong understanding of the anatomy and physiology of the visual system Experienced in mentoring and supervising undergraduate and graduate students in both laboratory and classroom settings Education # Ph.D. in Vision Science, University of California, Berkeley M.A. in Biological Psychology, Seoul National University, Korea B.A. in Psychology, Seoul National University, Korea Research experience # University of Washington, Washington National Primate Research Center # Acting Assistant Professor (Jan 2023 ‚Äì Present)\nInvestigating how the prefrontal cortex modulates feature selectivity in the visual cortex via inhibitory feedback Examining the functional architecture of brain areas in the ventral visual pathway using high-density electrophysiology and multi-photon Ca++ imaging in anesthetized macaques Exploring the neural mechanisms underlying visual crowding in behaving non-human primates Mentoring graduate students and post-docs in experimental design, data analysis, and programming (Matlab, Python) Acting Instructor (Oct 2019 ‚Äì Jan 2023)\nInvestigated the neural mechanisms underlying visual crowding effects using electrophysiology, psychophysics, and convolutional neural networks (published in J. Neurosci., 2024) Explored visual texture processing in the visual cortex with electrophysiology and machine learning, revealing that distinct texture sensations are linked to different temporal dynamics (published in J. Neurosci., 2022) Authored two review papers on visual information processing in the ventral visual pathway (published in Annu. Rev. Vis. Sci., 2020; Curr. Opin. Neurobiol., 2019) Mentored graduate students and research assistants on experimental design, data analysis (object segmentation, global motion processing), and programming (Matlab, Python) Senior Fellow (Oct 2015 ‚Äì Sep 2019)\nDevised metrics to quantify the perceptual qualities of natural texture images Studied how object shape and texture properties are jointly processed in the visual cortex using electrophysiology / computational modeling, finding that there are separate specializations in mid-level cortical processing for visual attributes of shape and texture (published in J. Neurosci., 2019) Advised and collaborated on a research project investigating neural correlates of global motion processing in the non-human primate visual cortex (published in Curr. Biol., 2023) University of California, Berkeley # Assistant Specialist (Jan 2015 ‚Äì Sep 2015)\nDesigned a human psychophysics experiment to demonstrate that binocular integration can occur during substantial differences in left and right eye signal strength. Wrote Matlab / Python code for visual stimulus generation, data acquisition, and analysis (published in Eur. J. Neurosci., 2017) Graduate Student Researcher (Aug 2010 ‚Äì Dec 2014)\nAnalyzed a database of cortical neurons to determine the degree of non-linearity of direction selectivity for cells within different laminae of the visual cortex (published in Eur. J. Neurosci., 2016) Investigated the effects of non-invasive transcranial magnetic stimulation (TMS) on functional tuning properties of visual cortical neurons (published in Brain Stimul., 2015) Conducted neurophysiological experiments to reveal segregated activity of feedforward, feedback, and horizontal pathways in visual cortex (published in Neuroscience, 2014) Led lab and discussion sessions for optometry students in Geometrical Optics class Seoul National University # Research Associate (Sep 2008 ‚Äì Jun 2010)\nStudied the spatiotemporal selectivity of V1 response using Gabor stimuli that were sequentially presented with a variable stimulus onset asynchrony. Wrote Matlab code for visual stimulus generation, data acquisition, and analysis (published in PLoS One, 2012; PLoS One, 2015) Graduate Student Researcher (Mar 2006 ‚Äì Aug 2008)\nConducted a human psychophysics study to examine the spatial localization error in visual short-term memory task (published in KCBPA, 2014) Led lab and discussion sessions for psychology students in Neuroscience and Biopsychology classes ","date":null,"permalink":"/about/","section":"","summary":"","title":""},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"}]